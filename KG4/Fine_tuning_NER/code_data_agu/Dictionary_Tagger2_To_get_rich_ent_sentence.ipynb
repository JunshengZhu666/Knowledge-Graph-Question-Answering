{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Raw_Corpus.txt ==========\n",
    "with open(\"Raw_Corpus_2021_12_20.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    training_corpus = f.read()\n",
    "\n",
    "# print(training_corpus[0:5])\n",
    "# print(len(training_corpus))\n",
    "# print()\n",
    "\n",
    "\n",
    "# remove some specical objects \n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# training_corpus = str(training_corpus)\n",
    "# training_corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", training_corpus)\n",
    "# training_corpus = training_corpus.lower()\n",
    "# print(training_corpus[0:10])\n",
    "# print(len(training_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316\n",
      "common metabolic problems that affect early postpartum cows such as retained fetal membranes, milk fever, ketosis, and displaced abomasum are know to extend the period of negative energy balance and delay resumption of ovarian cycles.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "tokenize_text = sent_tokenize(training_corpus)\n",
    "\n",
    "# print(len(tokenize_text))\n",
    "# print(tokenize_text[90:100])\n",
    "\n",
    "# lower \n",
    "new_tokenize_text = []\n",
    "for i in tokenize_text: \n",
    "    i = i.lower()\n",
    "    new_tokenize_text.append(i)\n",
    "    \n",
    "print(len(new_tokenize_text))\n",
    "print(new_tokenize_text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== Shuffle the dataset, split, and output to txt (for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316\n",
      "group cow relocations to only take place once a week.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "shuffle_all_sents = random.shuffle(new_tokenize_text)\n",
    "\n",
    "print(len(new_tokenize_text))\n",
    "print(new_tokenize_text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num1 = len(new_tokenize_text) * 7 // 10\n",
    "\n",
    "training_set = new_tokenize_text[:num1]\n",
    "testing_set = new_tokenize_text[num1:]\n",
    "\n",
    "print(len(training_set), len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert into the txt format\n",
    "\n",
    "def convert_to_raw_text(format_sent):\n",
    "    # convert to raw text \n",
    "    raw_rich_sent = \"\"\n",
    "    for i in format_sent:\n",
    "        raw_rich_sent += i + ' '\n",
    "    return raw_rich_sent\n",
    "\n",
    "training_set_txt = convert_to_raw_text(training_set)\n",
    "testing_set_txt = convert_to_raw_text(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output to the txt file \n",
    "\n",
    "### output to txt\n",
    "\n",
    "training_set_txt_file = open(\"training_set_txt_12_20.txt\", \"w\", encoding = 'utf-8')\n",
    "\n",
    "training_set_txt_file.write(training_set_txt)\n",
    "\n",
    "training_set_txt_file.close()\n",
    "\n",
    "### output to the txt file \n",
    "\n",
    "### output to txt\n",
    "\n",
    "testing_set_txt_file = open(\"testing_set_txt_12_20.txt\", \"w\", encoding = 'utf-8')\n",
    "\n",
    "testing_set_txt_file.write(testing_set_txt)\n",
    "\n",
    "testing_set_txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== load in dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acetate acid', 'acetoacetic acid', 'acetyl coenzyme', 'additives', 'alfalfa', 'alternate grazing', 'amino acid', 'amino acids', 'amylase', 'antibiotic']\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_NUT_2021_12_20.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "nutrition_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(nutrition_dict[0:10])\n",
    "print(len(nutrition_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abomasal displacement', 'abortion', 'acetonaemia', 'acorn toxicity', 'actinobacillosis', 'actinomycosis', 'acute rumen acidosis', 'ammoniated feeds', 'anoestrus', 'anthrax']\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_DIS_2021_12_20.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "disease_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(disease_dict[0:10])\n",
    "print(len(disease_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== replace the variable name here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenize_text = training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== expand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to get two and three combinations of entity words \n",
    "\n",
    "import re \n",
    "\n",
    "def get_two_three_comb(sentence):\n",
    "    \n",
    "        # remove punct \n",
    "    str1 = sentence.replace(',', '')\n",
    "    str1 = str1.replace('.', '')\n",
    "    \n",
    "    # split sentence\n",
    "    str1 = str1.split(' ')\n",
    "    \n",
    "    # for comb of two \n",
    "    comb_12 = []\n",
    "    for i, j in enumerate(str1):\n",
    "\n",
    "        comb12 = str1[i - 1] + \" \" + str1[i]\n",
    "        comb_12.append(comb12)\n",
    "        \n",
    "    # for comb of three \n",
    "    comb_13 = []\n",
    "    str1 = str1 + ['O']\n",
    "    comb_12 = ['O'] + comb_12\n",
    "    for i, j in zip(str1, comb_12): \n",
    "        comb13 = j + \" \"+ i\n",
    "        comb_13.append(comb13)  \n",
    "    \n",
    "    # final list \n",
    "    comb123 = str1 + comb_12 + comb_13\n",
    "    \n",
    "    return comb123\n",
    "\n",
    "# print(get_two_three_comb(words_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week', 'O', 'O', 'week group', 'group cow', 'cow relocations', 'relocations to', 'to only', 'only take', 'take place', 'place once', 'once a', 'a week', 'O group', 'week group cow', 'group cow relocations', 'cow relocations to', 'relocations to only', 'to only take', 'only take place', 'take place once', 'place once a', 'once a week', 'a week O']\n"
     ]
    }
   ],
   "source": [
    "expanded_word_list = []\n",
    "\n",
    "for i, j in enumerate(new_tokenize_text):\n",
    "    temp = get_two_three_comb(new_tokenize_text[i])\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    expanded_word_list.append(temp)\n",
    "    \n",
    "        \n",
    "# print(parsed_entities[i]['text'])\n",
    "\n",
    "print(len(expanded_word_list))\n",
    "print()\n",
    "print(expanded_word_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== get entity list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_for_a_sent(sent, dict1):\n",
    "    tagged_sent = []\n",
    "    for word in sent:\n",
    "        if word in dict1:\n",
    "            tagged_sent.append(word)\n",
    "            tagged_sent = list(tagged_sent)\n",
    "    return tagged_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ent_list_dis = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], disease_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_dis.append(temp1)\n",
    "    \n",
    "print(len(ent_list_dis))\n",
    "print(ent_list_dis[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for nutrition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['nfc', 'nfc']\n"
     ]
    }
   ],
   "source": [
    "ent_list_nut = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], nutrition_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_nut.append(temp1)\n",
    "    \n",
    "print(len(ent_list_nut))\n",
    "print(ent_list_nut[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== tag the origin list ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_dis(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'\n",
    "            \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "dis_tagged_sent = []\n",
    "\n",
    "for i, j in zip(t_sent_split, ent_list_dis):\n",
    "    temp2 = tag_dis(i,j)\n",
    "    dis_tagged_sent.append(temp2)\n",
    "    \n",
    "print(len(dis_tagged_sent))\n",
    "print(dis_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_nut(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'\n",
    "\n",
    "        except IndexError:\n",
    "            pass   \n",
    "        \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "nut_tagged_sent = []\n",
    "\n",
    "for i, j in zip(dis_tagged_sent, ent_list_nut):\n",
    "    temp4 = tag_nut(i,j)\n",
    "    nut_tagged_sent.append(temp4)\n",
    "    \n",
    "print(len(nut_tagged_sent))\n",
    "print(nut_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== concat two lists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for tag_sum_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25386\n",
      "['is', 'a', 'combination', 'of', 'controlled', 'research', 'observational', 'studies', 'and', 'field', 'experience', 'END', 'regardless', 'of', 'the', 'effect', 'on', 'rumen', 'epithelium', 'feedingdiets', 'containing', 'higher', 'proportions', 'of', 'B-NUT', 'should', 'pro-mote', 'ruminal', 'microbial', 'adaptation', 'to', 'B-NUT', 'levels', 'typicalof', 'lactation', 'diets']\n"
     ]
    }
   ],
   "source": [
    "tag_sum = []\n",
    "\n",
    "sent_append = []\n",
    "\n",
    "tag_sum_mid = []\n",
    "\n",
    "for sent in nut_tagged_sent:\n",
    "    sent_temp = sent + ['END']\n",
    "    sent_append.append(sent_temp)\n",
    "\n",
    "for sent in sent_append:\n",
    "    tag_sum_mid += sent\n",
    "    \n",
    "print(len(tag_sum_mid))\n",
    "print(tag_sum_mid[35:71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25386\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# replace with 'O'\n",
    "\n",
    "final_tag = []\n",
    "for index, i in enumerate(tag_sum_mid):\n",
    "    if i != 'B-DIS' and i != 'I-DIS' and i != 'B-NUT' and i != 'I-NUT' and i != 'END':\n",
    "        i = 'O'\n",
    "    else:\n",
    "        i = tag_sum_mid[index]\n",
    "    \n",
    "    final_tag.append(i)\n",
    "\n",
    "print(len(final_tag))\n",
    "print(final_tag[35:71])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code is to find rich sentence and augment them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [6, 4], [6], []]\n"
     ]
    }
   ],
   "source": [
    "### split a list by a value \n",
    "\n",
    "def split_by(test_list, value):\n",
    "    \n",
    "    # using list comprehension + zip() + slicing + enumerate()\n",
    "    # Split list into lists by particular value\n",
    "    size = len(test_list)\n",
    "    idx_list = [idx + 1 for idx, val in enumerate(test_list) if val == value]\n",
    "\n",
    "\n",
    "    res = [test_list[i: j-1] for i, j in\n",
    "            zip([0] + idx_list, idx_list + \n",
    "            ([size] if idx_list[-1] != size else []))]\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "test_list = [1, 4, 5, 6, 4, 5, 6, 5, 4]\n",
    "value = 5\n",
    "print(split_by(test_list, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== split_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "### split the tag\n",
    "\n",
    "split_tag = (split_by(final_tag, 'END'))\n",
    "\n",
    "print(len(split_tag))\n",
    "print(split_tag[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921\n",
      "['regardless', 'of', 'the', 'effect', 'on', 'rumen', 'epithelium', 'feedingdiets', 'containing', 'higher', 'proportions', 'of', 'nfc', 'should', 'pro-mote', 'ruminal', 'microbial', 'adaptation', 'to', 'nfc', 'levels', 'typicalof', 'lactation', 'diets']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here change the varibale name t_sent_split into split_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sent = t_sent_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So now we have split_tag and split_sent, we can find the sentence that contains many tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['the', 'objectives', 'here', 'were', 'to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals', 'sampled', 'which', 'were', 'above', 'nefa', 'and', 'bhb', 'critical', 'thresholds;', '4', 'evaluate', 'sampling', 'schemes', 'to', 'estimate', 'herd', 'level', 'outcomes;', '5', 'intensively', 'measure', 'the', 'incidence', 'of', 'early', 'lactation', 'subclinical', 'ketosis', 'in', 'high', 'performing', 'herds;', '6', 'identify', 'dry', 'period', 'risk', 'factors', 'for', 'cows', 'that', 'develop', 'ketosis;', 'and', '7', 'evaluate', 'the', 'cost:', 'benefit', 'of', 'various', 'testing', 'schemes', 'for', 'subclinical', 'ketosis', 'and', 'subsequent', 'treatment', 'of', 'positive', 'cows', 'with', 'propylene', 'glycol']\n",
      "338\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'B-NUT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'B-NUT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'I-NUT']\n"
     ]
    }
   ],
   "source": [
    "rich_sent = []\n",
    "rich_tag = []\n",
    "\n",
    "for i, j in zip(split_tag, split_sent):\n",
    "    num_ent = len([ent for ent in i if ent != 'O'])\n",
    "    if num_ent > 2: \n",
    "        rich_sent.append(j)\n",
    "        rich_tag.append(i)\n",
    "        \n",
    "print(len(rich_sent))\n",
    "print(rich_sent[0])\n",
    "\n",
    "print(len(rich_tag))\n",
    "print(rich_tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== To get rich sentence one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['the', 'objectives', 'here', 'were', 'to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals', 'sampled', 'which', 'were', 'above', 'nefa', 'and', 'bhb', 'critical', 'thresholds;', '4', 'evaluate', 'sampling', 'schemes', 'to', 'estimate', 'herd', 'level', 'outcomes;', '5', 'intensively', 'measure', 'the', 'incidence', 'of', 'early', 'lactation', 'subclinical', 'ketosis', 'in', 'high', 'performing', 'herds;', '6', 'identify', 'dry', 'period', 'risk', 'factors', 'for', 'cows', 'that', 'develop', 'ketosis;', 'and', '7', 'evaluate', 'the', 'cost:', 'benefit', 'of', 'various', 'testing', 'schemes', 'for', 'subclinical', 'ketosis', 'and', 'subsequent', 'treatment', 'of', 'positive', 'cows', 'with']\n",
      "\n",
      "10846\n",
      "['the', 'objectives', 'here', 'were', 'to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion']\n"
     ]
    }
   ],
   "source": [
    "### lost the last two words\n",
    "\n",
    "def lost_words(rich_sent, num):\n",
    "    rich_sent1s = []\n",
    "\n",
    "    ### remove last two words\n",
    "    for i,j in enumerate(rich_sent):\n",
    "        rich_sent1 = j[:num]\n",
    "        rich_sent1s.append(rich_sent1)\n",
    "    return rich_sent1s\n",
    "\n",
    "rich_sent1s = lost_words(rich_sent, -2)\n",
    "    \n",
    "print(len(rich_sent1s))\n",
    "print(rich_sent1s[0])\n",
    "\n",
    "def concat_with_END(t_sent_split):\n",
    "    ori_append = []\n",
    "\n",
    "    ori_sum_mid = []\n",
    "\n",
    "    # append 'END'\n",
    "    for k in t_sent_split:\n",
    "        ori_temp = k + ['END']\n",
    "        ori_append.append(ori_temp)\n",
    "\n",
    "    # concat all the sentence\n",
    "    for k in ori_append:\n",
    "        ori_sum_mid += k\n",
    "    return ori_sum_mid\n",
    "\n",
    "rich_sent_ones = concat_with_END(rich_sent1s)\n",
    "\n",
    "print()\n",
    "print(len(rich_sent_ones))\n",
    "print(rich_sent_ones[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== To get rich sentence two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['here', 'were', 'to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals', 'sampled', 'which', 'were', 'above', 'nefa', 'and', 'bhb', 'critical', 'thresholds;', '4', 'evaluate', 'sampling', 'schemes', 'to', 'estimate', 'herd', 'level', 'outcomes;', '5', 'intensively', 'measure', 'the', 'incidence', 'of', 'early', 'lactation', 'subclinical', 'ketosis', 'in', 'high', 'performing', 'herds;', '6', 'identify', 'dry', 'period', 'risk', 'factors', 'for', 'cows', 'that', 'develop', 'ketosis;', 'and', '7', 'evaluate', 'the', 'cost:', 'benefit', 'of', 'various', 'testing', 'schemes', 'for', 'subclinical', 'ketosis', 'and', 'subsequent', 'treatment', 'of', 'positive', 'cows', 'with', 'propylene', 'glycol']\n",
      "\n",
      "10846\n",
      "['here', 'were', 'to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals']\n"
     ]
    }
   ],
   "source": [
    "### lost the first two words\n",
    "\n",
    "def lost_words_two(rich_sent, num):\n",
    "    rich_sent1s = []\n",
    "\n",
    "    ### remove last two words\n",
    "    for i,j in enumerate(rich_sent):\n",
    "        rich_sent1 = j[num:]\n",
    "        rich_sent1s.append(rich_sent1)\n",
    "    return rich_sent1s\n",
    "\n",
    "rich_sent2s = lost_words_two(rich_sent, 2)\n",
    "    \n",
    "print(len(rich_sent2s))\n",
    "print(rich_sent2s[0])\n",
    "\n",
    "\n",
    "rich_sent_twos = concat_with_END(rich_sent2s)\n",
    "\n",
    "print()\n",
    "print(len(rich_sent_twos))\n",
    "print(rich_sent_twos[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== To get rich sentence three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals', 'sampled', 'which', 'were', 'above', 'nefa', 'and', 'bhb', 'critical', 'thresholds;', '4', 'evaluate', 'sampling', 'schemes', 'to', 'estimate', 'herd', 'level', 'outcomes;', '5', 'intensively', 'measure', 'the', 'incidence', 'of', 'early', 'lactation', 'subclinical', 'ketosis', 'in', 'high', 'performing', 'herds;', '6', 'identify', 'dry', 'period', 'risk', 'factors', 'for', 'cows', 'that', 'develop', 'ketosis;', 'and', '7', 'evaluate', 'the', 'cost:', 'benefit', 'of', 'various', 'testing', 'schemes', 'for', 'subclinical', 'ketosis', 'and', 'subsequent', 'treatment', 'of', 'positive']\n",
      "\n",
      "8819\n",
      "['to:', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the', 'individual', 'animal', 'level;', '2', 'investigate', 'the', 'magnitude', 'of', 'these', 'associations', 'in', 'free-stall', 'tmr-fed', 'herds;', '3', 'evaluate', 'herd-level', 'outcomes', 'associated', 'with', 'the', 'proportion', 'of', 'animals', 'sampled', 'which']\n"
     ]
    }
   ],
   "source": [
    "### lost the last and first two words\n",
    "\n",
    "def lost_words_three(rich_sent, num1, num2):\n",
    "    rich_sent1s = []\n",
    "\n",
    "    ### remove last two words\n",
    "    for i,j in enumerate(rich_sent):\n",
    "        rich_sent1 = j[num1:num2]\n",
    "        rich_sent1s.append(rich_sent1)\n",
    "    return rich_sent1s\n",
    "\n",
    "rich_sent3s = lost_words_three(rich_sent, 4, -4)\n",
    "    \n",
    "print(len(rich_sent3s))\n",
    "print(rich_sent3s[0])\n",
    "\n",
    "\n",
    "rich_sent_threes = concat_with_END(rich_sent3s)\n",
    "\n",
    "print()\n",
    "print(len(rich_sent_threes))\n",
    "print(rich_sent_threes[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rich_sent = rich_sent_ones + rich_sent_twos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== Convert into the sentence format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "the objectives here were to: 1 identify critical thresholds above which nefa and bhb concentrations increase the risk of disease and affect production and reproductive performance at the individual animal level; 2 investigate the magnitude of these associations in free-stall tmr-fed herds; 3 evaluate herd-level outcomes associated with the proportion of animals sampled which were above nefa and bhb critical thresholds; 4 evaluate sampling schemes to estimate herd level outcomes; 5 intensively measure the incidence of early lactation subclinical ketosis in high performing herds; 6 identify dry period risk factors for cows that develop ketosis; and 7 evaluate the cost: benefit of various testing schemes for subclinical ketosis and subsequent treatment of positive cows with .  conversely if each mastitis treatment were entered as a mastitis event the true incidence of mastitis would be .  roche (2003) demonstrated that even in regions of grazing cows where milk fever is well controlled th\n",
      "137551\n"
     ]
    }
   ],
   "source": [
    "format_sent = []\n",
    "\n",
    "for i in sum_rich_sent:\n",
    "    # convert 'END' to '. '\n",
    "    if i == 'END':\n",
    "        i = '. '\n",
    "    format_sent.append(i)\n",
    "    \n",
    "print(len(format_sent) == len(sum_rich_sent))\n",
    "\n",
    "# convert to raw text \n",
    "raw_rich_sent = \"\"\n",
    "for i in format_sent:\n",
    "    raw_rich_sent += i + ' '\n",
    "    \n",
    "print(raw_rich_sent[0:1000])\n",
    "print(len(raw_rich_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output to txt\n",
    "\n",
    "text_file = open(\"rich_sentence_from_training_set_12_20.txt\", \"w\", encoding = 'utf-8')\n",
    "\n",
    "text_file.write(raw_rich_sent)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
