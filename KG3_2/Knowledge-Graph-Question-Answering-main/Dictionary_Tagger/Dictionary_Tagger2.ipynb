{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Raw_Corpus.txt ==========\n",
    "with open(\"Raw_Corpus_2021_11_30.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    training_corpus = f.read()\n",
    "\n",
    "# print(training_corpus[0:5])\n",
    "# print(len(training_corpus))\n",
    "# print()\n",
    "\n",
    "\n",
    "# remove some specical objects \n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# training_corpus = str(training_corpus)\n",
    "# training_corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", training_corpus)\n",
    "# training_corpus = training_corpus.lower()\n",
    "# print(training_corpus[0:10])\n",
    "# print(len(training_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "common metabolic problems that affect early postpartum cows such as retained fetal membranes, milk fever, ketosis, and displaced abomasum are know to extend the period of negative energy balance and delay resumption of ovarian cycles.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "tokenize_text = sent_tokenize(training_corpus)\n",
    "\n",
    "# print(len(tokenize_text))\n",
    "# print(tokenize_text[90:100])\n",
    "\n",
    "# lower \n",
    "new_tokenize_text = []\n",
    "for i in tokenize_text: \n",
    "    i = i.lower()\n",
    "    new_tokenize_text.append(i)\n",
    "    \n",
    "print(len(new_tokenize_text))\n",
    "print(new_tokenize_text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== Dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acetate', 'acetate acid', 'acetoacetic acid', 'acetone', 'acetyl coenzyme a', 'additives', 'alfalfa', 'alternate grazing', 'amino acid', 'amino acids']\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_NUT_2021_12_07.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "nutrition_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(nutrition_dict[0:10])\n",
    "print(len(nutrition_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abomasal displacement', 'abortion', 'acetonaemia', 'acorn toxicity', 'actinobacillosis', 'actinomycosis', 'acute rumen acidosis', 'ammoniated feeds', 'anoestrus', 'anthrax']\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_DIS_2021_12_07.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "disease_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(disease_dict[0:10])\n",
    "print(len(disease_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== expand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to get two and three combinations of entity words \n",
    "\n",
    "import re \n",
    "\n",
    "def get_two_three_comb(sentence):\n",
    "    \n",
    "        # remove punct \n",
    "    str1 = sentence.replace(',', '')\n",
    "    str1 = str1.replace('.', '')\n",
    "    \n",
    "    # split sentence\n",
    "    str1 = str1.split(' ')\n",
    "    \n",
    "    # for comb of two \n",
    "    comb_12 = []\n",
    "    for i, j in enumerate(str1):\n",
    "\n",
    "        comb12 = str1[i - 1] + \" \" + str1[i]\n",
    "        comb_12.append(comb12)\n",
    "        \n",
    "    # for comb of three \n",
    "    comb_13 = []\n",
    "    str1 = str1 + ['O']\n",
    "    comb_12 = ['O'] + comb_12\n",
    "    for i, j in zip(str1, comb_12): \n",
    "        comb13 = j + \" \"+ i\n",
    "        comb_13.append(comb13)  \n",
    "    \n",
    "    # final list \n",
    "    comb123 = str1 + comb_12 + comb_13\n",
    "    \n",
    "    return comb123\n",
    "\n",
    "# print(get_two_three_comb(words_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "\n",
      "['common', 'metabolic', 'problems', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'retained', 'fetal', 'membranes', 'milk', 'fever', 'ketosis', 'and', 'displaced', 'abomasum', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'negative', 'energy', 'balance', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles', 'O', 'O', 'cycles common', 'common metabolic', 'metabolic problems', 'problems that', 'that affect', 'affect early', 'early postpartum', 'postpartum cows', 'cows such', 'such as', 'as retained', 'retained fetal', 'fetal membranes', 'membranes milk', 'milk fever', 'fever ketosis', 'ketosis and', 'and displaced', 'displaced abomasum', 'abomasum are', 'are know', 'know to', 'to extend', 'extend the', 'the period', 'period of', 'of negative', 'negative energy', 'energy balance', 'balance and', 'and delay', 'delay resumption', 'resumption of', 'of ovarian', 'ovarian cycles', 'O common', 'cycles common metabolic', 'common metabolic problems', 'metabolic problems that', 'problems that affect', 'that affect early', 'affect early postpartum', 'early postpartum cows', 'postpartum cows such', 'cows such as', 'such as retained', 'as retained fetal', 'retained fetal membranes', 'fetal membranes milk', 'membranes milk fever', 'milk fever ketosis', 'fever ketosis and', 'ketosis and displaced', 'and displaced abomasum', 'displaced abomasum are', 'abomasum are know', 'are know to', 'know to extend', 'to extend the', 'extend the period', 'the period of', 'period of negative', 'of negative energy', 'negative energy balance', 'energy balance and', 'balance and delay', 'and delay resumption', 'delay resumption of', 'resumption of ovarian', 'of ovarian cycles', 'ovarian cycles O']\n"
     ]
    }
   ],
   "source": [
    "expanded_word_list = []\n",
    "\n",
    "for i, j in enumerate(new_tokenize_text):\n",
    "    temp = get_two_three_comb(new_tokenize_text[i])\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    expanded_word_list.append(temp)\n",
    "    \n",
    "        \n",
    "# print(parsed_entities[i]['text'])\n",
    "\n",
    "print(len(expanded_word_list))\n",
    "print()\n",
    "print(expanded_word_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== get entity list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_for_a_sent(sent, dict1):\n",
    "    tagged_sent = []\n",
    "    for word in sent:\n",
    "        if word in dict1:\n",
    "            tagged_sent.append(word)\n",
    "            tagged_sent = list(tagged_sent)\n",
    "    return tagged_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['ketosis', 'metabolic problems', 'milk fever', 'displaced abomasum', 'retained fetal membranes', 'negative energy balance']\n"
     ]
    }
   ],
   "source": [
    "ent_list_dis = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], disease_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_dis.append(temp1)\n",
    "    \n",
    "print(len(ent_list_dis))\n",
    "print(ent_list_dis[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for nutrition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['nonesterified', 'triacylglycerol', 'fatty acids']\n"
     ]
    }
   ],
   "source": [
    "ent_list_nut = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], nutrition_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_nut.append(temp1)\n",
    "    \n",
    "print(len(ent_list_nut))\n",
    "print(ent_list_nut[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== tag the origin list ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['common', 'metabolic', 'problems', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'retained', 'fetal', 'membranes', 'milk', 'fever', 'ketosis', 'and', 'displaced', 'abomasum', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'negative', 'energy', 'balance', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_dis(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'\n",
    "            \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['common', 'B-DIS', 'I-DIS', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'B-DIS', 'I-DIS', 'I-DIS', 'B-DIS', 'I-DIS', 'B-DIS', 'and', 'B-DIS', 'I-DIS', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'B-DIS', 'I-DIS', 'I-DIS', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles']\n"
     ]
    }
   ],
   "source": [
    "dis_tagged_sent = []\n",
    "\n",
    "for i, j in zip(t_sent_split, ent_list_dis):\n",
    "    temp2 = tag_dis(i,j)\n",
    "    dis_tagged_sent.append(temp2)\n",
    "    \n",
    "print(len(dis_tagged_sent))\n",
    "print(dis_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_nut(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'\n",
    "\n",
    "        except IndexError:\n",
    "            pass   \n",
    "        \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['common', 'B-DIS', 'I-DIS', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'B-DIS', 'I-DIS', 'I-DIS', 'B-DIS', 'I-DIS', 'B-DIS', 'and', 'B-DIS', 'I-DIS', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'B-DIS', 'I-DIS', 'I-DIS', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles']\n"
     ]
    }
   ],
   "source": [
    "nut_tagged_sent = []\n",
    "\n",
    "for i, j in zip(dis_tagged_sent, ent_list_nut):\n",
    "    temp4 = tag_nut(i,j)\n",
    "    nut_tagged_sent.append(temp4)\n",
    "    \n",
    "print(len(nut_tagged_sent))\n",
    "print(nut_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat two lists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for tag_sum_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30936\n",
      "['common', 'B-DIS', 'I-DIS', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'B-DIS', 'I-DIS', 'I-DIS', 'B-DIS', 'I-DIS', 'B-DIS', 'and', 'B-DIS', 'I-DIS', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'B-DIS', 'I-DIS', 'I-DIS', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles', 'END']\n"
     ]
    }
   ],
   "source": [
    "tag_sum = []\n",
    "\n",
    "sent_append = []\n",
    "\n",
    "tag_sum_mid = []\n",
    "\n",
    "for sent in nut_tagged_sent:\n",
    "    sent_temp = sent + ['END']\n",
    "    sent_append.append(sent_temp)\n",
    "\n",
    "for sent in sent_append:\n",
    "    tag_sum_mid += sent\n",
    "    \n",
    "print(len(tag_sum_mid))\n",
    "print(tag_sum_mid[35:71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30936\n",
      "['O', 'B-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DIS', 'I-DIS', 'I-DIS', 'B-DIS', 'I-DIS', 'B-DIS', 'O', 'B-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DIS', 'I-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n"
     ]
    }
   ],
   "source": [
    "# replace with 'O'\n",
    "\n",
    "final_tag = []\n",
    "for index, i in enumerate(tag_sum_mid):\n",
    "    if i != 'B-DIS' and i != 'I-DIS' and i != 'B-NUT' and i != 'I-NUT' and i != 'END':\n",
    "        i = 'O'\n",
    "    else:\n",
    "        i = tag_sum_mid[index]\n",
    "    \n",
    "    final_tag.append(i)\n",
    "\n",
    "print(len(final_tag))\n",
    "print(final_tag[35:71])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for origin sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145\n",
      "['cows', 'fed', 'high', 'fermentable', 'energy', 'diets', 'prepartum', 'have', 'improved', 'energy', 'balance', 'reduced', 'concentrations', 'of', 'plasma', 'nonesterified', 'fatty', 'acids', 'and', 'ß-hydroxybutyrate', 'and', 'reduced', 'triacylglycerol', 'infiltration', 'in', 'the', 'hepatic', 'tissue']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30936\n",
      "['literature', 'linking', 'the', 'effects', 'of', 'prepartum', 'nutrition', 'and', 'subsequent', 'fertility', 'is', 'scarce', 'END', 'most', 'of', 'what', 'is', 'suggested', 'to', 'optimize', 'future', 'fertility', 'is', 'related', 'to', 'relationships', 'between', 'metabolic', 'disorders', 'and', 'risk', 'for', 'delayed', 'conception', 'END', 'common', 'metabolic', 'problems', 'that', 'affect', 'early', 'postpartum', 'cows', 'such', 'as', 'retained', 'fetal', 'membranes', 'milk', 'fever', 'ketosis', 'and', 'displaced', 'abomasum', 'are', 'know', 'to', 'extend', 'the', 'period', 'of', 'negative', 'energy', 'balance', 'and', 'delay', 'resumption', 'of', 'ovarian', 'cycles', 'END', 'manipulation', 'of', 'the', 'energy', 'content', 'of', 'the', 'diet', 'prepartum', 'has', 'been', 'shown', 'to', 'affect', 'dry', 'matter', 'intake', '(hayirli', 'et', 'al', '2002)', 'and', 'postpartum', 'lactational', 'performance', 'END', 'cows', 'fed', 'high']\n"
     ]
    }
   ],
   "source": [
    "t_sent_split\n",
    "\n",
    "ori_append = []\n",
    "\n",
    "ori_sum_mid = []\n",
    "\n",
    "for k in t_sent_split:\n",
    "    ori_temp = k + ['END']\n",
    "    ori_append.append(ori_temp)\n",
    "\n",
    "for k in ori_append:\n",
    "    ori_sum_mid += k\n",
    "    \n",
    "print(len(ori_sum_mid))\n",
    "print(ori_sum_mid[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, final_tag\n",
    "# 2, ori_sum_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as words, labels and pos\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "df = pd.DataFrame([ori_sum_mid,final_tag])\n",
    "\n",
    "# transpose the columns and rows\n",
    "d_f = df.T\n",
    "\n",
    "# ========== Name of the output excel  ==========\n",
    "\n",
    "d_f.to_excel('Tagged_corpus_2021_12_07_1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
