{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/54473254/cudnnlstm-unknownerror-fail-to-find-the-dnn-implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the random seed \n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "import random \n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, Load and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extracting the entities and tags\n",
    "\n",
    "import os \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def split_text_label(filename):\n",
    "    '''\n",
    "    Reads a file named filename, extracts the text and the labels and stores\n",
    "    them in an array.\n",
    "     \n",
    "    returns [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ] \n",
    "    '''\n",
    "    \n",
    "    # open file\n",
    "    f = open(filename)\n",
    "    \n",
    "    # initializing\n",
    "    split_labeled_text = []\n",
    "    sentence = []\n",
    "    \n",
    "    # processing line by line \n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\t\":\n",
    "            if len(sentence) > 0:\n",
    "                split_labeled_text.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        \n",
    "        # split by tab\n",
    "        splits = line.split('\t')\n",
    "        # rstrip: strip from the right \n",
    "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
    "        \n",
    "        \n",
    "    if len(sentence) > 0:\n",
    "        split_labeled_text.append(sentence)\n",
    "        sentence = []\n",
    "    return split_labeled_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Raw_Corpus and split into train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all data:  1152\n",
      "\n",
      "Length of training data:  922\n",
      "Length of validing data:  230\n",
      "Length of testing data:  230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load in data\n",
    "all_data = split_text_label(os.path.join(\"Tagged_corpus_2021_12_07_1.txt\"))\n",
    "print(\"Length of all data: \", len(all_data))\n",
    "print()\n",
    "\n",
    "# shuffle all the data \n",
    "import random \n",
    "random.seed(4)\n",
    "shuffle_data = random.shuffle(all_data)\n",
    "\n",
    "# split into train, valid, test by precentage\n",
    "train_num = round(len(all_data) * 0.8)\n",
    "valid_num = round(len(all_data) * 0.2)\n",
    "# test_num = round(len(all_data) * 0.2)\n",
    "\n",
    "# here we do not use cross validation, just use train and valid with 4:1\n",
    "split_train = all_data[:train_num]\n",
    "split_valid = all_data[train_num:]\n",
    "split_test = all_data[train_num:]\n",
    "\n",
    "print(\"Length of training data: \", len(split_train))\n",
    "print(\"Length of validing data: \", len(split_valid))\n",
    "print(\"Length of testing data: \", len(split_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===== here to test half of training data\n",
    "\n",
    "split_train = split_train[:461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  [[['consider', 'O'], ['using', 'O'], ['a', 'O'], ['teat', 'O'], ['sealant', 'O']]]\n",
      "\n",
      "validing data:  [[['when', 'O'], ['implemented', 'O'], ['correctly', 'O'], ['these', 'O'], ['diets', 'O'], ['have', 'O'], ['been', 'O'], ['successful', 'O'], ['in', 'O'], ['decreasing', 'O'], ['incidences', 'O'], ['of', 'O'], ['peripartal', 'O'], ['health', 'B-DIS'], ['disorders', 'I-DIS']]]\n",
      "\n",
      "testing data:  [[['when', 'O'], ['implemented', 'O'], ['correctly', 'O'], ['these', 'O'], ['diets', 'O'], ['have', 'O'], ['been', 'O'], ['successful', 'O'], ['in', 'O'], ['decreasing', 'O'], ['incidences', 'O'], ['of', 'O'], ['peripartal', 'O'], ['health', 'B-DIS'], ['disorders', 'I-DIS']]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some list to see \n",
    "\n",
    "print(\"training data: \", split_train[0:1])\n",
    "print()\n",
    "\n",
    "print(\"validing data: \", split_valid[0:1])\n",
    "print()\n",
    "\n",
    "print(\"testing data: \", split_test[0:1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, Buidling Vocabulary\n",
    "\n",
    "### build a vocabulary for the text, so we can assign a unique index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labelSet:  5\n",
      "labelSet:  {'B-NUT', 'O', 'I-DIS', 'I-NUT', 'B-DIS'}\n",
      "\n",
      "wordSet:  3133\n"
     ]
    }
   ],
   "source": [
    "### use labelSet() as label dictionary \n",
    "### use wordSet() as word dictionary \n",
    "\n",
    "labelSet = set()\n",
    "wordSet = set()\n",
    "# words and labels\n",
    "for data in [split_train, split_valid, split_test]:\n",
    "    for labeled_text in data:\n",
    "        for word, label in labeled_text:\n",
    "            \n",
    "            # modifying unwanted mistag\n",
    "            if label == 'i-NUT':\n",
    "                label = 'I-NUT'\n",
    "            if label == '':\n",
    "                label = 'O'\n",
    "            \n",
    "            \n",
    "            labelSet.add(label)\n",
    "            wordSet.add(word.lower())\n",
    "            \n",
    "\n",
    "\n",
    "# modifying the label set \n",
    "print()\n",
    "#empty = labelSet.pop()\n",
    "\n",
    "labelSet.remove('\"performance')\n",
    "labelSet.remove('\"results')\n",
    "# labelSet.remove('\"cow')\n",
    "labelSet.remove('\"cows')\n",
    "labelSet.remove('\"ration')\n",
    "labelSet.remove('\"cattle')\n",
    "# labelSet.remove('\"2000)')\n",
    "labelSet.remove('\"in')\n",
    "labelSet.remove('\"')\n",
    "labelSet.remove('\"e')\n",
    "labelSet.remove('\"disorders')\n",
    "labelSet.remove('\"staggers')\n",
    "    \n",
    "\n",
    "\n",
    "# check for the len \n",
    "print(\"labelSet: \", len(labelSet))\n",
    "print(\"labelSet: \", labelSet)\n",
    "print()\n",
    "print(\"wordSet: \", len(wordSet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, Assiging index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2Label:  5\n",
      "idx2Label:  {0: 'O', 1: 'B-NUT', 2: 'I-DIS', 3: 'I-NUT', 4: 'B-DIS'}\n",
      "\n",
      "word2Idx:  3135\n",
      "word2Idx:  {'PADDING_TOKEN': 0, 'UNKNOWN_TOKEN': 1, 'pronounced': 2, 'decades;': 3, 'placed': 4, '\"staggers\\n': 5, '—is': 6, 'currently': 7, 'again': 8, 'parturient': 9, 'extensive': 10, 'intensive': 11, 'peak': 12, 'dr': 13, 'digestion': 14, 'breakdown': 15, 'conversely': 16, '9': 17, 'dairies': 18, '(68': 19, 'least': 20, 'drench': 21, 'seriously': 22, 'goal': 23, '(1992)': 24, 'productivity/': 25, '(condition)': 26, '$290': 27, 'first': 28, 'constituents': 29, '1974;': 30, 'ad': 31, 'absence': 32, 'circulating': 33, 'poorly': 34, 'abomasum': 35, 'acetoacetate': 36, 'macromineral': 37, 'second': 38, 'uterine': 39, 'mammogenesis': 40, 'included': 41, '(seifi': 42, 'period)': 43, 'ruminal': 44, 'hypophosphite': 45, 'choice': 46, 'identify': 47, 'three': 48, 'differ': 49, 'role': 50, 'saturated': 51, 'waldron': 52, 'kg]': 53, 'producers': 54, 'high-straw': 55, 'loss38': 56, 'year': 57, '(marcos': 58, 'life': 59, 'lessen': 60, 'ufl': 61, 'targets': 62, '(dirksen': 63, 'quick': 64, 'deposited': 65, '(1998)': 66, 'limiting': 67, 'prolonged': 68, 'favoured': 69, 'active': 70, 'insulin': 71, 'pathological': 72, 'consideration': 73, 'increased': 74, 'associated': 75, 'lactational': 76, 'underfed': 77, 'consumed)': 78, '5%': 79, 'axes': 80, 'drackley': 81, 'article': 82, 'health': 83, 'soon': 84, '(me]': 85, 'metabolic': 86, 'grains': 87, '22': 88, 'massive': 89, 'smoothly': 90, 'do': 91, 'enriched': 92, 'beings': 93, 'whether': 94, 'technological': 95, 'entered': 96, 'two': 97, '(fas)': 98, 'µmol/l': 99, 'would': 100, '3kg': 101, 'spent': 102, 'of': 103, 'demonstrated': 104, 'livers': 105, 'less': 106, 'eating': 107, '130%': 108, 'zealand': 109, 'regulation': 110, 'cautioned': 111, 'latter': 112, '039)': 113, 'hypocalcemia\"': 114, 'steam-flaked': 115, 'palatability': 116, 'alone': 117, 'recent': 118, 'borogluconate': 119, 'ca': 120, '1976)': 121, 'fuels': 122, 'association': 123, 'cycles': 124, 'feeding': 125, '15': 126, 'beginning': 127, '(rajala-schultz': 128, 'level': 129, 'comprehensive': 130, 'catabolism': 131, 'feces': 132, 'cereals': 133, 'surpasses': 134, 'scheme': 135, 'economic': 136, '1989)': 137, 'regard': 138, '(11)': 139, 'cow/early': 140, 'effectiveness': 141, 'terms': 142, 'designed': 143, 'calves': 144, 'interval': 145, 'particular': 146, 'leading': 147, 'averaging': 148, '2-3': 149, 'conduction': 150, 'neutrophils': 151, 'serve': 152, 'her': 153, 'sizes33': 154, 'essential': 155, 'may': 156, 'process': 157, 'considered': 158, 'actionable': 159, 'mid-gestation': 160, '26': 161, 'based': 162, 'definitions': 163, 'urea': 164, 'advantage': 165, 'bw': 166, '(coppock': 167, 'experienced': 168, 'sun”': 169, 'versus': 170, '(owens': 171, 'diagnosis': 172, '1992)': 173, 'unrealized': 174, 'advised': 175, 'emergence': 176, 'accurate': 177, '(andersen': 178, '230': 179, 'communication;': 180, 'silage': 181, 'earlier': 182, '\"cows\\n': 183, 'optimum': 184, 'letting': 185, 'plus': 186, 'intestine': 187, 'set': 188, 'failure': 189, 'free-stall': 190, 'intramuscularly': 191, 'dynamics': 192, 'following': 193, 'predisposing': 194, 'reason': 195, 'acids': 196, 'chloride': 197, 'regrouping': 198, 'grouping/facility': 199, 'guess': 200, 'up': 201, 'elevations': 202, 'pre-partum': 203, 'all': 204, '1993': 205, 'fact': 206, 'resulting': 207, 'hyperketonemia': 208, 'digestibility': 209, 'appear': 210, 'benchmarks': 211, 'implying': 212, 'dohoo': 213, 'russell': 214, 'routine': 215, 'direct': 216, 'me': 217, '24': 218, 'improvements': 219, 'lowered': 220, 'after': 221, 'fat;': 222, 'access': 223, 'ventral': 224, '$128': 225, 'third': 226, '(ie': 227, 'logistically': 228, 'field': 229, 'normal': 230, 'absorption': 231, 'impacts': 232, 'reduced': 233, 'training': 234, 'µmol/l)': 235, '?hydroxybutyrate': 236, '150': 237, 'muscle': 238, 'ketone': 239, 'living': 240, 'domino': 241, 'matter': 242, 'explained': 243, 'move': 244, 'calibrate': 245, 'estimate': 246, 'accurately': 247, '(10)': 248, '?': 249, 'models': 250, 'subclinical': 251, 'imbalancing': 252, 'abomasal': 253, '(fatty': 254, 'benefiting': 255, '08': 256, 'results': 257, 'survey': 258, '225': 259, 'healthy': 260, 'above6-32': 261, 'profitable': 262, 'neutralized': 263, 'prevention': 264, 'design—retrospective': 265, 'mechanisms': 266, \"it's\": 267, 'cracked': 268, 'stop': 269, 'depressed;': 270, 'warranted': 271, 'neb-related': 272, 'regions': 273, 'without': 274, 'kg/d)': 275, 'bacterial': 276, 'hypomagnesaemic': 277, '\"results\\n': 278, 'assisting': 279, 'pg': 280, 'acidified': 281, 'disorders': 282, 'metrics': 283, 'largely': 284, 'primiparous': 285, 'mcal/day': 286, 'prevalence': 287, 'metabolize': 288, '(eg': 289, 'accumulation': 290, 'yield': 291, 'aspects': 292, '4]': 293, 'facility': 294, 'once': 295, 'recognition': 296, 'rumen': 297, 'undegradable': 298, '03': 299, 'supplied': 300, 'relapses': 301, 'strive': 302, '$50000': 303, 'scenarios': 304, '(dohoo': 305, 'moderate': 306, 'clean': 307, 'sound': 308, 'ovarian': 309, 'developed': 310, 'physological': 311, 'and/or': 312, 'dmd)': 313, 'yielding': 314, 'urine': 315, 'component': 316, 'turnover': 317, 'fates:': 318, '80%': 319, '\"performance\\n': 320, 'estimates': 321, '(4': 322, 'vitamins': 323, 'separate': 324, 'indeed': 325, 'any': 326, 'covariates': 327, 'grummer': 328, 'decreasing': 329, 'leads': 330, 'disappointing': 331, 'program': 332, 'weekly': 333, 'people': 334, 'adipocytes': 335, 'insidious': 336, 'lipid': 337, '(smith': 338, 'separating': 339, '33%': 340, '1992);': 341, 'differences': 342, 'knowledge': 343, '20%': 344, 'propionic': 345, 'enhanced': 346, 'principal': 347, 'appropriate': 348, 'poorer': 349, '(low': 350, 'compare': 351, 'site': 352, 'treatment': 353, 'manipulate': 354, 'take': 355, '(kunz': 356, 'indirectly': 357, 'screening': 358, 'month': 359, 'holsteins': 360, 'department': 361, 'drop': 362, 'tion': 363, '(massey': 364, 'much': 365, '446': 366, '(christenson': 367, 'vaccinated': 368, 'certainly': 369, 'obese': 370, 'hand': 371, 'america': 372, 'discussion': 373, 'facilities': 374, '149': 375, 'documented': 376, 'studies': 377, 'nonnutritional': 378, 'release': 379, 'samples': 380, 'reviewed': 381, '(1997)': 382, 'later': 383, 'axis': 384, 'manage': 385, 'divided': 386, '(bauman': 387, 'chlorinated': 388, '07': 389, 'intakes': 390, 'prepare': 391, 'nonfibrous': 392, 'effect': 393, 'sping': 394, 'recumbent': 395, 'postpartum': 396, 'lying': 397, 'demonstrating': 398, 'observations': 399, '049)': 400, 'negative': 401, 'accounting': 402, 'allows': 403, 'numerous': 404, 'decision': 405, 'imbalance': 406, 'uncoupling': 407, 'experiment': 408, 'far': 409, 'sealant': 410, '(sara)': 411, 'true': 412, '(stephenson': 413, 'must': 414, 'sampled': 415, '123': 416, 'exception': 417, 'day': 418, 'measurements': 419, 'could': 420, 'collectively': 421, '2009)': 422, 'around': 423, 'lameness': 424, 'presumably': 425, 'atting': 426, 'information': 427, 'cameron': 428, 'frequency': 429, 'formulation': 430, 'early': 431, 'weight': 432, 'met': 433, 'milk': 434, 'predisposes': 435, 'deping': 436, 'vitamin': 437, 'states': 438, 'homeostasis': 439, 'peaking': 440, 'conservative': 441, 'adverse': 442, 'resistance': 443, 'does': 444, 'fat:protein': 445, 'did': 446, '3534kg': 447, 'conception': 448, 'responses': 449, '1985)': 450, 'been': 451, 'determine': 452, 'swedish': 453, 'control': 454, 'at-risk': 455, '5000l': 456, '“risk': 457, 'non-esterified': 458, 'jorritsma': 459, 'grossly': 460, 'overall': 461, 'integrative': 462, 'losses': 463, '–': 464, 'pathophysiological': 465, 'decreases': 466, 'pounds': 467, 'caused': 468, 'high-risk': 469, '335': 470, 'other': 471, 'secretion': 472, 'readily': 473, 'recounted': 474, 'increase': 475, 'we': 476, 'metabolizable': 477, 'scientific': 478, '(barragan': 479, 'cost': 480, 'pregnancy': 481, 'necessary': 482, 'systemic': 483, 'cellulolytic': 484, 'difficult': 485, 'fat)': 486, 'receive': 487, 'specifically': 488, 'itself': 489, 'prenatally': 490, 'bhb': 491, 'driven': 492, '(0-60dim)': 493, 'concomitant': 494, 'systems': 495, 'require': 496, 'critical': 497, 'soybean': 498, 'under-reported': 499, '1996)': 500, 'particles': 501, 'starches': 502, 'unusual': 503, '1990;': 504, 'administration': 505, 'about': 506, 'mass': 507, 'intake;': 508, 'management-related': 509, 'varying': 510, 'meq/l;': 511, 'stillborn': 512, 'conclusion': 513, 'mallard': 514, '2015': 515, 'eosinophil': 516, 'space': 517, 'persistency': 518, 'reference': 519, 'unwilling': 520, '(milk': 521, 'martin': 522, 'position': 523, '2000;': 524, '1': 525, 'their': 526, 'b)': 527, '72': 528, 'meal': 529, 'citing': 530, 'weeks': 531, 'especially': 532, 'showed': 533, 'supports': 534, 'minimize': 535, 'managed': 536, 'case': 537, 'treating': 538, 'posttreatment': 539, 'received': 540, 'slow;': 541, 'indicators': 542, '(miettenan': 543, 'implications': 544, 'test-and-treat': 545, 'et': 546, 'plays': 547, 'practices': 548, 'cook': 549, '50%': 550, 'category': 551, 'variable': 552, 'prominent': 553, 'completely': 554, 'mounting': 555, 'restricted': 556, 'already': 557, 'consuming)': 558, '(2010)': 559, '25%': 560, 'aspect': 561, 'plagued': 562, 'a\"': 563, '(mertens': 564, 'being': 565, 'might': 566, '12': 567, 'renewed': 568, '475': 569, 'liver;': 570, 'represent': 571, 'seven': 572, 'syndrome': 573, '“close-up”': 574, 'has': 575, 'improved': 576, 'calving-related': 577, 'group-housed': 578, 'an': 579, 'randomised': 580, 'limited': 581, 'fetus': 582, 'data': 583, 'surgical': 584, 'overconditioning': 585, '80%-85%': 586, 'course': 587, 'microminerals': 588, 'avoiding': 589, 'fas': 590, 'pre-calving': 591, 'reasons': 592, 'cost:': 593, 'optimizing': 594, 'abnormalities': 595, 'hope-cawdery': 596, ':': 597, 'this': 598, 'estrous': 599, '81ufl': 600, 'lipids': 601, 'artificial': 602, 'equally': 603, 'named': 604, 'aa': 605, 'post': 606, 'lactating': 607, 'objectives': 608, 'northeast': 609, 'older': 610, '“steamup”': 611, 'bedding': 612, 'post-partum': 613, 'mean': 614, 'triglyceride': 615, 'sires': 616, 'affects': 617, 'operations': 618, '130': 619, 'dynamic': 620, 'suggesting': 621, '(staples': 622, 'maximal': 623, 'bertics': 624, 'surprisingly': 625, 'worthy': 626, 'side': 627, 'postpartum;': 628, 'ketosis)': 629, 'pregnant': 630, 'bodies': 631, 'santos': 632, 'inevitable': 633, '38': 634, '(1213)': 635, 'support': 636, 'elaborate': 637, 'among': 638, 'exciting': 639, 'define': 640, 'accommodate': 641, 'occur': 642, 'measuring': 643, '(acetyl': 644, 'linear': 645, 'experience': 646, 'into': 647, 'm': 648, 'sections': 649, 'chronic': 650, 'abomasum—factors': 651, 'found': 652, 'going': 653, 'feedstuffs': 654, 'delayed': 655, 'hyperglycaemic': 656, 'markers': 657, 'serum': 658, 'consistent': 659, 'rogers': 660, 'traditional': 661, '(bhb)': 662, '\"syndrome\"\"\"': 663, 'confirm': 664, '48%': 665, 'laboratory': 666, 'giving': 667, 'timeframe': 668, '21': 669, 'entire': 670, 'volume': 671, 'confidence': 672, 'lot': 673, 'articles': 674, 'time': 675, 'circulation': 676, 'was': 677, 'valuable': 678, 'dm': 679, 'groups:': 680, 'interest': 681, 'tissues11': 682, '(nocek': 683, 'reproduction': 684, 'degradabilities': 685, 'lead': 686, 'genetically': 687, 'six': 688, '7': 689, 'dmd': 690, 'tool': 691, 'lactation': 692, 'cation': 693, '36': 694, 'returns': 695, 'essentially': 696, '(16': 697, 'lost': 698, '(acetone': 699, 'phases:': 700, 'fatty': 701, 'calf44': 702, 'triglycerides': 703, '200': 704, 'moves': 705, '011)': 706, 'pre': 707, 'multivariate': 708, 'suggest': 709, 'ineffective': 710, 'dangerous': 711, '400kg': 712, '4000': 713, 'one': 714, 'non-nutritional': 715, 'patterns': 716, '(1985)': 717, '2020c)': 718, 'equal': 719, 'blood': 720, 'decrease': 721, 'spite': 722, 'silages': 723, 'possible': 724, '(ca': 725, 'falls': 726, 'continues': 727, 'either': 728, '$700': 729, 'range': 730, '500': 731, 'slowly': 732, 'liberated': 733, 'bias': 734, 'insight': 735, 'variety': 736, 'measurement': 737, 'potentially': 738, 'challenging': 739, 'curves': 740, 'functions': 741, '44': 742, 'respond': 743, 'areas': 744, 'load”': 745, 'both': 746, 'muscular': 747, 'bhba': 748, 'chopping': 749, 'horst': 750, 'ocrinology': 751, 'adaptation': 752, '(starch)': 753, 'studying': 754, 'over¬': 755, 'quantitatively': 756, 'result1415': 757, 'within': 758, 'individual': 759, '18%': 760, 'robert': 761, 'considerable': 762, 'ionophores': 763, 'cessation': 764, 'often': 765, '2020)': 766, 'intervention': 767, 'treatments': 768, 'disturbed': 769, 'e': 770, '(dcad)': 771, 'energy': 772, 'fed;': 773, 'midlactation': 774, '(mp}': 775, 'trimester': 776, 'physiological': 777, 'enormous': 778, 'measure': 779, 'tonicity': 780, 'ketotic': 781, 'sch': 782, 'contraction)': 783, 'prior': 784, 'enter': 785, 'promoting': 786, 'rumen-inert': 787, 'forage': 788, 'underlie': 789, 'scientists': 790, '(andersson': 791, 'increases': 792, '(apoprotein': 793, 'produce': 794, 'controlling': 795, '(2002)': 796, '417': 797, 'event': 798, 'laboratories': 799, '1)': 800, 'amino': 801, 'that': 802, 'immune': 803, 'certain': 804, 'concurrently': 805, 'overton': 806, 'neb18': 807, 'most': 808, 'point': 809, 'marginal': 810, 'expression': 811, 'weights': 812, 'compromises': 813, 'analyses': 814, 'disappear': 815, 'coordinate': 816, 'tetany': 817, 'council': 818, 'scc': 819, 'many': 820, 'infections': 821, 'lack': 822, 'diets': 823, '(bertics': 824, 'continued': 825, 'particularly': 826, 'recognize': 827, 'middlings': 828, 'highly': 829, 'overestimated': 830, 'concentrate': 831, 'attributable': 832, 'adapted': 833, '(2006)': 834, 'passage': 835, 'milk/d': 836, 'lanna': 837, 'coming': 838, 'becoming': 839, '39': 840, 'actually': 841, 'year36': 842, 'smith': 843, 'struggle': 844, 'contrast': 845, '(dann': 846, 'identified': 847, 'topic': 848, 'glucose': 849, 'its': 850, 'severe': 851, 'commingling': 852, 'nevertheless': 853, 'achieved;': 854, 'restriction': 855, 'put': 856, 'period;': 857, 'dried': 858, 'experimental': 859, 'decreased': 860, 'expected': 861, 'total': 862, 'quantity': 863, 'step': 864, 'tiestalls': 865, 'poses': 866, 'agriculture': 867, 'model': 868, 'fast': 869, 'keeping': 870, 'activation': 871, 'ruminants': 872, 'unable': 873, 'enthusiastically': 874, 'postcalving': 875, 'identifying': 876, 'house': 877, 'runs': 878, 'realize': 879, '46': 880, '>12': 881, 'high': 882, 'wolter': 883, 'circumstances': 884, 'herds)': 885, 'effective': 886, 'variations': 887, 'deliver': 888, 'enough': 889, 'costly': 890, 'application': 891, 'ospina': 892, 'even': 893, 'aggressive': 894, 'exists': 895, 'considering': 896, 'takes': 897, 'controversial': 898, 'examinations': 899, 'principally': 900, 'pitfalls': 901, 'scope': 902, 'natural': 903, 'etiology': 904, 'accrued': 905, 'proportions': 906, 'capacity': 907, 'productivity': 908, 'indicative': 909, 'transitions': 910, 'new': 911, 'continue': 912, 'cells': 913, 'vet': 914, 'contain': 915, 'bolus': 916, 'personal': 917, 'popular-based': 918, 'thresholds;': 919, 'minimal': 920, 'crucial': 921, 'analogous': 922, 'advances': 923, 'separator': 924, 'ketones': 925, 'tao': 926, 'byproducts': 927, 'recommations': 928, 'transition-related': 929, 'periods8': 930, '2000': 931, 'timing': 932, 'hulls': 933, '1980': 934, 'term': 935, 'rapidly': 936, 'away': 937, 'organisations': 938, '(box': 939, 'proportionately': 940, 'none': 941, 'ranges': 942, 'achieving': 943, '\"in\\n': 944, '208': 945, 'stall': 946, '(5-point': 947, 'supplemental': 948, 'risks': 949, '30%': 950, 'influence': 951, 'cows40': 952, 'long-term': 953, 'exceed': 954, '2002;': 955, 'douglas': 956, '2': 957, 'minimization': 958, 'examining': 959, 'borchardt': 960, 'sensitivity': 961, 'cereal': 962, 'balanced': 963, 'events': 964, 'mobilize': 965, 'organic': 966, 'crude': 967, 'important': 968, 'contraction]': 969, '(holtenius': 970, 'examination': 971, 'fermentation': 972, 'occurred': 973, 'drewry': 974, 'first-parity': 975, 'techniques': 976, 'progression': 977, '(duffield': 978, 'evaluated': 979, 'highlighting': 980, 'catabolic': 981, 'reticulorumen': 982, 'periods': 983, 'subsequent': 984, 'reserves': 985, 'exhibited': 986, 'supplementary': 987, 'microbial': 988, 'suggests': 989, 'type': 990, '1995': 991, 'group)': 992, 'fruitful': 993, 'competition': 994, 'at': 995, 'developing': 996, 'provides': 997, '(serum': 998, 'particle': 999, 'lipase': 1000, 'cannot': 1001, 'properties': 1002, 'profitability': 1003, 'demand': 1004, 'feed': 1005, 'fresh-cow': 1006, 'size)': 1007, 'impact': 1008, 'masking': 1009, 'modulating': 1010, 'blueprints': 1011, 'somatotropin': 1012, 'high-producing': 1013, 'niacin': 1014, 'have': 1015, 'anionic': 1016, 'sampling': 1017, 'allow': 1018, 'solids': 1019, 'numerically': 1020, 'option': 1021, 'highlights': 1022, 'bell': 1023, 'allowed': 1024, 'makes': 1025, 'season': 1026, 'effects': 1027, 'mg/dl': 1028, '(high': 1029, 'risk': 1030, 'cow8]': 1031, 'halt': 1032, '1100': 1033, 'fever316': 1034, 'response': 1035, 'conduct': 1036, 'service6': 1037, '(2000)': 1038, 'opportunities': 1039, 'drive': 1040, 'amass': 1041, '(national': 1042, 'conventional': 1043, 'recommed': 1044, 'controllable': 1045, 'fates': 1046, 'beyond': 1047, 'standard': 1048, '>15': 1049, 'multiparous': 1050, 'method': 1051, '(1982)': 1052, 'downstream': 1053, 'parenteral': 1054, 'respiratory': 1055, 'farms': 1056, 'ß-hydroxybutyrate': 1057, 'contributing': 1058, 'them': 1059, 'manifested': 1060, '1998;': 1061, 'remit': 1062, 'flowing': 1063, 'pulp': 1064, 'bone': 1065, 'body': 1066, '(douglas': 1067, 'overconsumption': 1068, 'guidelines': 1069, 'i': 1070, 'aids': 1071, 'microbes': 1072, 'lactogenesis': 1073, '(emery': 1074, 'ranged': 1075, '1994)': 1076, 'placentas': 1077, 'posing': 1078, 'measures': 1079, 'low-energy': 1080, '07mmol/l': 1081, 'relative': 1082, 'recognised': 1083, 'united': 1084, 'lipidosis': 1085, 'animals—1038': 1086, 'indicate': 1087, '(drackley': 1088, 'involve': 1089, 'excessive': 1090, 'treated': 1091, 'exist': 1092, 'ratio': 1093, '(ingvartsen': 1094, 'reesterifying': 1095, 'counter': 1096, 'increasing': 1097, 'van': 1098, 'peripartal': 1099, 'best': 1100, '(p': 1101, 'to:': 1102, 'therefore': 1103, 'uncommon': 1104, 'begin': 1105, 'meals': 1106, 'noticeable': 1107, 'no': 1108, 'oxidation': 1109, 'from': 1110, 'thoughts': 1111, 'suppressing': 1112, 'status': 1113, 'improvement': 1114, 'kehrli': 1115, 'between': 1116, '31': 1117, 'extracellular': 1118, 'hundred': 1119, 'supplementing': 1120, '1kg': 1121, 'top': 1122, '4': 1123, 'fluids': 1124, 'aid': 1125, 'levels': 1126, 'correctly': 1127, 'practice': 1128, 'cows;': 1129, 'target': 1130, '60': 1131, 'papillae': 1132, 'evaluate': 1133, 'evaluation': 1134, '1997;': 1135, 'bloodstream': 1136, '(1)': 1137, 'regardless': 1138, 'mixing': 1139, 'to': 1140, 'previous': 1141, '(shaver': 1142, 'through': 1143, 'sensitive': 1144, 'motility': 1145, '(zahra': 1146, '71': 1147, \"cow's\": 1148, 'important)': 1149, 'done': 1150, 'develop': 1151, 'efficacious': 1152, 'drops': 1153, '(1980)': 1154, 'stand': 1155, 'minerals': 1156, 'output': 1157, 'calve': 1158, '(dcad]': 1159, 'reach': 1160, 'lymphocyte': 1161, 'replicated': 1162, 'p)': 1163, '125': 1164, 'cubicle': 1165, 'centuries': 1166, '5-15kg': 1167, 'forage-to-grain': 1168, '[bhb])': 1169, 'factors': 1170, 'animal': 1171, 'diffusion': 1172, 'limitation': 1173, 'costs3': 1174, 'cm]': 1175, 'exclusively': 1176, '06': 1177, 'marked': 1178, 'significantly': 1179, 'efficacy': 1180, 'lose': 1181, 'compounds': 1182, 'infusion': 1183, 'similarly': 1184, 'described': 1185, '(cameron': 1186, 'precipitate': 1187, 'see': 1188, 'herds;': 1189, 'synthesized': 1190, 'review': 1191, '10-12': 1192, 'farm': 1193, 'had': 1194, 'cycling': 1195, 'times': 1196, 'what': 1197, 'implementation)': 1198, 'assigning': 1199, 'report': 1200, 'recomm': 1201, 'fundamental': 1202, 'vandehaar': 1203, 'ones': 1204, 'kinetics': 1205, 'containing': 1206, 'lactic': 1207, 'subsequently': 1208, 'abomasums)': 1209, 'thus': 1210, 'learn': 1211, 'mineral': 1212, 'freestall': 1213, 'ph': 1214, 'produced': 1215, 'movement': 1216, 'welfare': 1217, 'positive': 1218, 'resulted': 1219, 'assess': 1220, 'initial': 1221, 'blood-brain': 1222, 'variables': 1223, '(dmi)': 1224, 'issues': 1225, 'abdomen': 1226, 'precursors': 1227, 'signs': 1228, 'digestive': 1229, 'postpartal': 1230, 'dysregulation': 1231, 'accumulate': 1232, 'insemination39': 1233, 'correlated': 1234, 'outlined': 1235, 'per¬': 1236, 'da)': 1237, '25': 1238, '?-hydroxybutyrate': 1239, 'consider': 1240, 'below': 1241, 'outcomes;': 1242, 'accompanied': 1243, 'required': 1244, '(reid': 1245, 'conjunction': 1246, 'over-conditioning': 1247, 'evident': 1248, '(461': 1249, 'dining': 1250, 'stimulate': 1251, 'fat': 1252, 'practiced': 1253, 'predictors': 1254, 'use': 1255, '(sck)': 1256, '14': 1257, 'forty-nine': 1258, 'differently': 1259, 'closeup': 1260, 'treat-all-cows': 1261, '$11728': 1262, 'physiologic': 1263, '(nfc)': 1264, 'relocations': 1265, '2001)': 1266, 'breeding': 1267, 'bcs': 1268, '(4)': 1269, 'mechanistic': 1270, 'g/d': 1271, 'proportion': 1272, '?tocopherol': 1273, 'benefit': 1274, 'outcomes': 1275, 'stores': 1276, '7%': 1277, 'modern': 1278, 'cent': 1279, 'visible': 1280, 'prophylactic': 1281, 'immediately': 1282, 'throughout': 1283, 'unchanged': 1284, 'amylolytic': 1285, 'or': 1286, 'parasites': 1287, 'others': 1288, '(use': 1289, 'gram-positive': 1290, 'placenta': 1291, '(pdi)': 1292, 'protocols': 1293, 'b-vitamins': 1294, 'clearly': 1295, '01': 1296, 'relatively': 1297, 'experiencing': 1298, 'gradually': 1299, '(o’mara': 1300, 'high-fiber': 1301, '1991;': 1302, 'normally': 1303, 'disodium': 1304, 'temperature': 1305, 'large': 1306, 'appropriately': 1307, 'predominantly': 1308, 'calculated': 1309, 'labor': 1310, 'markedly': 1311, 'full': 1312, 'technician': 1313, 'mark': 1314, 'reduce': 1315, 'rp': 1316, 'likely': 1317, 'protozoa': 1318, 'fats': 1319, 'acclimatise': 1320, 'tissues': 1321, 'modify': 1322, 'acid': 1323, 'resumption': 1324, 'available': 1325, 'crc': 1326, 'complex': 1327, 'nonfirbrous': 1328, 'before': 1329, 'by': 1330, 'saliva': 1331, 'adequate': 1332, 'se': 1333, 'animals': 1334, 'mastitis41': 1335, 'prepared': 1336, 'findings': 1337, 'loss;': 1338, 'curve': 1339, '(constable': 1340, 'performing': 1341, 'impede': 1342, '2003;': 1343, 'said': 1344, 'requires': 1345, 'above)': 1346, 'basic': 1347, 'production': 1348, 'though': 1349, '32': 1350, 'acetone)': 1351, 'antiketogenic': 1352, '1976;': 1353, 'tg': 1354, '1984)': 1355, 'commonly': 1356, 'standards': 1357, 'damage': 1358, 'enters': 1359, 'higher-energy': 1360, '$316': 1361, 'months': 1362, 'paresis': 1363, 'benefits': 1364, 'monitoring': 1365, '40%': 1366, 'mg': 1367, 'whitaker': 1368, 'having': 1369, 'under': 1370, 'compared': 1371, 'topdress': 1372, 'affect': 1373, 'occurrence': 1374, 'trace': 1375, 'get': 1376, '(loor': 1377, 'agenäs': 1378, '(4%': 1379, 'death': 1380, 'l': 1381, 'rates': 1382, 'inadequate': 1383, 'primigravid': 1384, 'frustrating': 1385, 'randomized': 1386, 'emphasis': 1387, '2005)': 1388, 'yields': 1389, 'generally': 1390, 'coenzyme': 1391, 'tuesday': 1392, 'non-fiber': 1393, 'genetic': 1394, 'phospholipids': 1395, 'system': 1396, 'corrected': 1397, 'approach': 1398, 'checking': 1399, \"today's\": 1400, 'reviews': 1401, 'counts': 1402, 'intake': 1403, 'fronk': 1404, 'correlations': 1405, 'become': 1406, 'isoflupredone': 1407, 'midstream': 1408, 'loss)': 1409, 'delivers': 1410, 'corn': 1411, 'holstein': 1412, '001)': 1413, 'where': 1414, 'logistic': 1415, 'adult': 1416, '(morrow': 1417, 'tied': 1418, 'primary': 1419, 'tools': 1420, 'titis': 1421, '(van': 1422, 'stages': 1423, 'contents': 1424, 'grazing': 1425, 'immense': 1426, 'primarily': 1427, '(nefas)': 1428, '161': 1429, '(08': 1430, 'finally': 1431, 'gene': 1432, 'recently': 1433, '(blanc': 1434, 'concentrations': 1435, '(pipenbrink': 1436, '399': 1437, 'herds': 1438, 'penn': 1439, 'dm/d': 1440, 'surrounding': 1441, 'milieu': 1442, 'reported': 1443, 'dm)': 1444, 'net': 1445, 'allotment': 1446, 'gabel': 1447, 'obtained': 1448, 'stories': 1449, '50': 1450, 'severity': 1451, 'multiple': 1452, 'early-lactation': 1453, 'hammon': 1454, '(crc)': 1455, 'decades': 1456, '>14mmol/l': 1457, 'far-off': 1458, 'substances': 1459, 'social': 1460, 'collection': 1461, '$444': 1462, 'on': 1463, 'short-term': 1464, 'visibly': 1465, 'can': 1466, 'lda': 1467, 'mobilisation': 1468, 'generates': 1469, 'abatement': 1470, 'cholesterol': 1471, 'd-depent': 1472, 'push-ups': 1473, 'bulkiness': 1474, 'colostrum': 1475, '(p=009)': 1476, 'methods': 1477, 'csf': 1478, 'appears': 1479, 'inexpensive': 1480, 'although': 1481, 'grain': 1482, 'fresh': 1483, 'introduction\"': 1484, 'underestimated': 1485, 'doses': 1486, 'unclear': 1487, 'propylene': 1488, 'dcad)': 1489, 'synthesize': 1490, '(fig': 1491, 'unlikely': 1492, 'fewer': 1493, 'react': 1494, '48': 1495, 'stressors': 1496, 'random': 1497, 'well-being': 1498, 'late': 1499, 'provided': 1500, 'ensure': 1501, '165': 1502, 'stress': 1503, 'expulsion': 1504, '(dm': 1505, 'related': 1506, 'elusive': 1507, 'doubled': 1508, 'seen': 1509, 'assessing': 1510, 'eat': 1511, 'consequence': 1512, 'home': 1513, 'approximately': 1514, 'efficiently': 1515, 'our': 1516, 'bunk': 1517, 'accumulated': 1518, '\"\"\"fat\"': 1519, 'investigations': 1520, 'liver': 1521, 'implicated': 1522, 'ruminally': 1523, 'twins': 1524, 'beede': 1525, 'arbitrary': 1526, 'treat': 1527, 'providing': 1528, 'pit': 1529, 'burke': 1530, 'extremely': 1531, '2%': 1532, 'conducive': 1533, 'external': 1534, 'achieve': 1535, '11\"': 1536, 'implemented': 1537, 'off': 1538, 'acetate': 1539, 'week': 1540, 'return': 1541, 'wallace': 1542, 'maximize': 1543, '(33%': 1544, 'reid': 1545, 'predispose': 1546, 'interestingly': 1547, '64': 1548, 'fitted': 1549, 'four': 1550, 'elevation': 1551, 'size': 1552, 'perfect': 1553, 'preventive': 1554, '157%': 1555, 'go': 1556, 'assuming': 1557, 'meq/100': 1558, 'technology': 1559, '(3': 1560, 'acidosis': 1561, 'considerations': 1562, 'technologies': 1563, 'abomasum;': 1564, '(nefa)': 1565, 'changed': 1566, 'smaller': 1567, 'comfortable': 1568, '(n': 1569, 'discarded': 1570, 'reaction': 1571, 'add': 1572, 'emanuelson': 1573, '105': 1574, 'a)': 1575, 'fatness': 1576, 'competence': 1577, '[both': 1578, 'projects': 1579, 'northern': 1580, 'therapy': 1581, 'elimination': 1582, 'outside': 1583, 'sedatives': 1584, 'equivalent': 1585, 'aminotransferase': 1586, 'basis': 1587, 'similar': 1588, 'post-calving': 1589, 'krebs': 1590, 'pen': 1591, 'substrates': 1592, 'mas¬': 1593, 'wherein': 1594, 'cow-level': 1595, 'neutrophil': 1596, 'awareness': 1597, 'heifers': 1598, 'faster': 1599, 'legume': 1600, 'widely': 1601, 'long': 1602, 'inappropriate': 1603, '(lda)': 1604, '2004)': 1605, 'postpartum)': 1606, '[sch)': 1607, 'bi-directional': 1608, 'cannulas': 1609, 'addressed': 1610, 'increased\"': 1611, 'own': 1612, '020': 1613, '(1984)': 1614, 'lowest': 1615, 'hormonal': 1616, 'let': 1617, 'macro-': 1618, 'dim': 1619, 'displaced': 1620, 'long-chain': 1621, 'those': 1622, 'various': 1623, 'supply': 1624, '“decreasing”': 1625, 'helpful': 1626, 'rare': 1627, 'summary': 1628, 'adding': 1629, 'amounts': 1630, 'tmr-fed': 1631, 'mild': 1632, 'miettenen': 1633, \"herd's\": 1634, 'calving': 1635, 'disorder': 1636, 'advocated': 1637, 'uses': 1638, 'hypothesis': 1639, 'fetal': 1640, 'controlled-energy': 1641, 'intravenous': 1642, 'deposition': 1643, 'cardiac': 1644, '<': 1645, 'woweeks': 1646, 'culminate': 1647, 'published': 1648, '(grummer': 1649, 'concepts': 1650, 'duration': 1651, 'observation': 1652, 'seasonal': 1653, 'dairy\"': 1654, 'allowances': 1655, 'cur¬': 1656, '034)': 1657, 'mcal': 1658, 'facilitated': 1659, '(because': 1660, '(grass': 1661, 'moderately': 1662, 'monocyte': 1663, 'upper': 1664, 'gestation': 1665, 'diet': 1666, '06m': 1667, 'simulate': 1668, 'upon': 1669, 'decline': 1670, 'place': 1671, 'which': 1672, '(goff': 1673, 'conducted': 1674, 'weight;': 1675, 'gram-negative': 1676, '1400': 1677, '(kauppinen': 1678, 'source': 1679, 'sodium': 1680, 'fuel': 1681, 'developmental': 1682, 'actual': 1683, 'beneficial': 1684, 'indoors': 1685, 'reesterified': 1686, 'administered': 1687, 'useful': 1688, 'complicating': 1689, 'removal': 1690, 'animals32': 1691, '2001;': 1692, 'management': 1693, 'glands': 1694, 'than': 1695, 'preventative': 1696, 'becomes': 1697, 'overconsumed': 1698, 'dim)': 1699, '(r': 1700, 'quantities': 1701, 'too': 1702, 'little': 1703, '2016a': 1704, 'glucocorticoids': 1705, 'pushing': 1706, 'subjectively': 1707, '(da)': 1708, 'inferences': 1709, '1980)': 1710, 'also': 1711, 'moving': 1712, 'verify': 1713, 'sara': 1714, 'force-feeding': 1715, 'significant': 1716, 'indepently': 1717, 'defense': 1718, '1998)': 1719, 'prevent': 1720, 'same': 1721, 'flies': 1722, '410': 1723, 'separately': 1724, 'composition': 1725, '(oetzel': 1726, 'ketosis': 1727, 'occurring': 1728, '(both': 1729, 'ideas': 1730, 'detrimental': 1731, 'calcium': 1732, 'grum': 1733, 'characteristics': 1734, '(2003)': 1735, 'glucogenic': 1736, \"potential5'25\": 1737, 'consistently': 1738, 'depicted': 1739, 'ted': 1740, 'triggers': 1741, 'imperative': 1742, 'include': 1743, '6%': 1744, 'perhaps': 1745, 'they': 1746, 'the': 1747, 'common': 1748, 'drying': 1749, 'related;': 1750, 'component-fed': 1751, 'maximise': 1752, 'recovery': 1753, 'divide': 1754, 'condition': 1755, 'hypocalcemia)': 1756, 'extent': 1757, '8': 1758, 'desirable': 1759, '(not': 1760, 'impaired': 1761, '100': 1762, 'dmi;': 1763, 'regression': 1764, 'monogastrics': 1765, 'frequently': 1766, 'strategies': 1767, 'investigators4–8': 1768, 'explore': 1769, 'logical': 1770, 'for': 1771, 'infection': 1772, '(neb)': 1773, 'den': 1774, '“vfa': 1775, 'half-century': 1776, 'gather': 1777, 'injectable': 1778, 'fourth': 1779, 'synthesis': 1780, 'recheck': 1781, '(grohn': 1782, 'herd)': 1783, 'detection': 1784, 'indirect': 1785, 'particularities': 1786, 'metritis': 1787, 'retained': 1788, 'demands': 1789, 'mobilized': 1790, 'accepted': 1791, 'elevated': 1792, 'working': 1793, 'will': 1794, 'minor': 1795, '13': 1796, 'approaches': 1797, 'authors': 1798, 'low-rank': 1799, 'traditionally': 1800, '(1995)': 1801, '1993)': 1802, '(for': 1803, 'lacks': 1804, 'dry-off': 1805, 'corrections': 1806, 'testing': 1807, 'thought': 1808, 'tmr': 1809, 'adopted': 1810, '2016a)': 1811, 'due': 1812, 'non-lactating': 1813, '0': 1814, 'abnormal': 1815, 'counterproductive': 1816, 'sufficient': 1817, '?-tocopherol': 1818, 'close-up': 1819, 'postnatally': 1820, 'suggestions': 1821, 'capsule': 1822, 'such': 1823, 'consume': 1824, '95%': 1825, 'yet': 1826, 'lowquality': 1827, 'respectively': 1828, 'work': 1829, 'offered': 1830, 'substantial': 1831, 'relates': 1832, 'induce': 1833, 'should': 1834, 'how': 1835, 'ketogenic': 1836, 'insufficient': 1837, 'individually': 1838, 'combinations': 1839, '<305': 1840, 'dcad': 1841, 'current': 1842, 'fiber': 1843, 'good-quality': 1844, 'delivery': 1845, 'export': 1846, 'spot': 1847, 'reductions': 1848, 'choline': 1849, 'basis)': 1850, 'kremer': 1851, 'metabolism': 1852, 'exercise': 1853, 'sfc': 1854, 'employing': 1855, 'emery': 1856, 'gluconeogenic': 1857, '05': 1858, '30-325': 1859, 'iv': 1860, 'dystocia': 1861, 'criteria': 1862, 'huge': 1863, 'intensity': 1864, 'part': 1865, 'allowing': 1866, 'intense': 1867, 'since': 1868, '2003ab;': 1869, '(steen': 1870, 'be': 1871, 'ideally': 1872, 'hyperketonemic': 1873, 'recognized': 1874, 'approximate': 1875, 'experiences': 1876, '2020b)': 1877, 'nationwide': 1878, 'made': 1879, 'vaccination': 1880, 'solutions': 1881, 'grass': 1882, 'nutritional': 1883, '2006)': 1884, '1928': 1885, 'dry\"': 1886, 'study': 1887, 'window': 1888, 'some': 1889, 'placental': 1890, 'mat': 1891, 'show': 1892, 'aetiology': 1893, 'contributes': 1894, 'programmed': 1895, 'economics': 1896, 'maximizing': 1897, 'irish': 1898, 'favor': 1899, 'services': 1900, 'concentrates': 1901, 'design': 1902, 'immunity': 1903, 'shortening': 1904, 'f': 1905, '30': 1906, 'level;': 1907, 'low': 1908, 'inability': 1909, 'rent': 1910, 'performance': 1911, 'industry': 1912, 'problems': 1913, 'concerns': 1914, 'integrated': 1915, 'phosphate': 1916, 'fever': 1917, 'meet': 1918, 'lacking': 1919, 'performed': 1920, 'better': 1921, 'facing': 1922, 'start': 1923, 'result': 1924, '\"ration\\n': 1925, 'mmol/l)': 1926, '>25%': 1927, 'efficiency': 1928, 'aiming': 1929, 'ability': 1930, 'threshold': 1931, 'subcutaneous': 1932, 'literature': 1933, 'severe/recurrent': 1934, '(kung': 1935, 'mixed': 1936, 'focused': 1937, 'trial': 1938, 'remain': 1939, 'elsewhere33': 1940, 'are': 1941, 'oltenacu': 1942, '(tatone': 1943, 'improve': 1944, 'intestinal': 1945, 'thereby': 1946, 'closely': 1947, 'importance': 1948, '(2)': 1949, '94%': 1950, '(mann': 1951, 'estimation': 1952, 'mastitis': 1953, 'plasma': 1954, 'select': 1955, 'paper': 1956, 'activity': 1957, 'pretreatment': 1958, 'analysis': 1959, 'abruptly': 1960, 'managing': 1961, 'causes': 1962, 'less)': 1963, '(if': 1964, '%': 1965, 'cl': 1966, 'pay': 1967, '43%': 1968, 'stalls': 1969, 'animal’s': 1970, 'outcomes:': 1971, 'million': 1972, '600kg': 1973, 'proceed': 1974, 'odds': 1975, '(250': 1976, 'stage': 1977, 'deficit': 1978, 'post-fresh': 1979, 'future': 1980, 'resorption': 1981, 'midwest': 1982, '16': 1983, 'fermentable': 1984, 'problem': 1985, 'higher': 1986, 'density;': 1987, 'rumensin': 1988, 'are:': 1989, '2020a)': 1990, 'requirement': 1991, 'concept': 1992, 'almost': 1993, 'consumption': 1994, 'optimal': 1995, 'carefully': 1996, 'additives': 1997, 'so-called': 1998, 'leukocyte': 1999, 'compromised': 2000, 'consult': 2001, 'accommodation': 2002, 'body’s': 2003, 'close': 2004, '(g': 2005, 'distribution': 2006, 'significant)': 2007, 'making': 2008, 'controls': 2009, 'enzymes': 2010, '(garnsworthy': 2011, '&': 2012, 'summarize': 2013, 'reasonable': 2014, 'development': 2015, \"kronfeld's\": 2016, '3)': 2017, 'several': 2018, '1996;': 2019, '(and': 2020, 'appreciable': 2021, 'setting': 2022, '507': 2023, 'pathways': 2024, 'favors': 2025, 'success': 2026, 'meaningful': 2027, 'rumination': 2028, 'using': 2029, 'undergo': 2030, 'ration': 2031, 'intensively': 2032, 'combined': 2033, 'then': 2034, 'consecutive': 2035, 'ruminants\"': 2036, 'averaged': 2037, 'injection': 2038, 'caution': 2039, 'ingvartsen': 2040, 'notable': 2041, 'reduces': 2042, 'consequently': 2043, 'drink': 2044, 'displacement': 2045, 'saun': 2046, 'challenges': 2047, 'hypocalcemia': 2048, 'short': 2049, 'exceeds': 2050, 'across': 2051, 'vfa': 2052, 'helps': 2053, 'epithelium': 2054, 'management721': 2055, 'strongest': 2056, 'visit': 2057, 'variation': 2058, 'linoleic': 2059, 'involution': 2060, 'absorb': 2061, '\"\"\"gap\"\"\"': 2062, '?mol/l': 2063, 'easier': 2064, 'g': 2065, 'subacute': 2066, 'behavior': 2067, 'overconditioned': 2068, 'fertility': 2069, 'dam': 2070, 'depression': 2071, 'interplay': 2072, '(1': 2073, 'headlocks': 2074, 'help': 2075, 'productive': 2076, 'policy': 2077, 'pass': 2078, 'state': 2079, '(2001)': 2080, 'infectious': 2081, '0-60': 2082, 'indicates': 2083, 'feeds': 2084, 'big': 2085, 'cause': 2086, '\"disorders\\n': 2087, 'densities': 2088, 'partial': 2089, 'teat': 2090, '35': 2091, '280': 2092, 'recommation': 2093, 'fight': 2094, 'metabolites': 2095, 'ruoff': 2096, '1997': 2097, 'partition': 2098, 'communication)': 2099, 'appetite': 2100, 'nonfibous': 2101, 'detected': 2102, 'retrospective': 2103, 'cattle': 2104, 'and': 2105, 'general': 2106, 'additionally': 2107, 'gained': 2108, 'permit': 2109, 'nfc': 2110, 'barrier': 2111, 'period': 2112, 'stocking': 2113, 'allowance': 2114, 'responsible': 2115, 'existence': 2116, 'us': 2117, 'small': 2118, 'identification': 2119, 'substance': 2120, 'ndf': 2121, '1900': 2122, 'beet': 2123, 'immediate': 2124, 'concentration': 2125, 'advise': 2126, 'calf': 2127, 'ratios': 2128, 'twinning': 2129, 'involved': 2130, 'not': 2131, 'forages': 2132, 'formulated': 2133, 'components': 2134, 'amounted': 2135, 'tissue-specific': 2136, '63': 2137, 'most\"': 2138, '2002)': 2139, 'expressed': 2140, 'right': 2141, 'majority': 2142, '“pre-fresh”': 2143, 'prophylaxis': 2144, 'mp': 2145, '(agenäs': 2146, 'minimizing': 2147, 'lipoprotein': 2148, 'importantly': 2149, 'reliable': 2150, '8kg': 2151, 'length': 2152, '(holcomb': 2153, 'water': 2154, 'assume': 2155, '15%': 2156, 'your': 2157, 'k': 2158, 'formulate': 2159, 'organs': 2160, '18': 2161, 'presented': 2162, 'created': 2163, 'affected': 2164, 'now': 2165, 'opportunity': 2166, 'associations': 2167, 'suffering': 2168, 'frequent': 2169, 'martens': 2170, 'sources': 2171, '(total': 2172, 'interrelated': 2173, 'interested': 2174, 'whatever': 2175, '2003)': 2176, 'think': 2177, 'like': 2178, 'therapeutic': 2179, 'days': 2180, 'withdraw': 2181, 'alfalfa': 2182, 'relationships': 2183, 'distinct': 2184, '(18': 2185, 'density': 2186, '(dim)': 2187, '(valacta': 2188, 'commercial': 2189, 'medical': 2190, 'standardised': 2191, 'conflicting': 2192, 'faced': 2193, 'misinterpreted': 2194, 'continuous': 2195, 'oral': 2196, 'inflammatory': 2197, 'test': 2198, '2000)': 2199, 'somatotropic': 2200, 'normalise': 2201, 'omnipresent': 2202, 'costs;': 2203, 'unit': 2204, 'mcnamara': 2205, 'used': 2206, 'units': 2207, 'addition': 2208, 'par-turition': 2209, 'provide': 2210, 'diagnoses': 2211, '031': 2212, 'seems': 2213, 'focus': 2214, 'look': 2215, 'daily': 2216, 'composed': 2217, 'twice': 2218, 'glycogen': 2219, 'out': 2220, 'classic': 2221, 'potassium': 2222, 'definition': 2223, '300': 2224, 'holcomb': 2225, '15-50': 2226, 'occurs': 2227, '(herdt': 2228, 'internally': 2229, 'heat': 2230, 'inverted': 2231, '(miettenen': 2232, 'h': 2233, 'but': 2234, 'able': 2235, 'additional': 2236, '1980;': 2237, 'sense': 2238, 'crowding': 2239, 'scale)': 2240, 'ireland': 2241, 'controlled': 2242, 'minutes': 2243, '28': 2244, '(pg)': 2245, 'immunosuppression': 2246, 'protected': 2247, 'libitum': 2248, 'coupled': 2249, 'interactions': 2250, '1989': 2251, '(vannucchi': 2252, 'facilitate': 2253, 'remainder': 2254, 'nordlund': 2255, 'quality': 2256, 'carbohydrate': 2257, '(tmr)': 2258, 'ways': 2259, 'requirements': 2260, 'relation': 2261, 'hours': 2262, 'if': 2263, '1984;': 2264, 'thresholds': 2265, 'responders': 2266, 'qualities': 2267, 'interventions': 2268, 'base': 2269, 'difference': 2270, 'metabolite': 2271, 'rather': 2272, 'with': 2273, 'another': 2274, 'inclusion': 2275, 'present': 2276, 'nutrition': 2277, 'it': 2278, 'pdi': 2279, 'kauppinen': 2280, 'probably': 2281, 'thinner': 2282, 'dose': 2283, '(bhb': 2284, 'untreated': 2285, 'respectivelythese': 2286, 'agents': 2287, 'undergoes': 2288, 'restore': 2289, 'centage': 2290, '(especially': 2291, 'calving)': 2292, 'acid;': 2293, 'underscore': 2294, '10': 2295, 'observed': 2296, 'roche': 2297, '20': 2298, 'high-bulk': 2299, 'trivial': 2300, 'rukkwamsuk': 2301, 'staggering': 2302, 'maintenance': 2303, 'cohort': 2304, 'ancillary': 2305, '(600kg)': 2306, 'thermoregulation': 2307, '(900': 2308, 'peripartum': 2309, 'post¬': 2310, 'exted': 2311, 'analysed': 2312, 'large-herd': 2313, 'remained': 2314, 'kidney': 2315, 'until': 2316, 'surface': 2317, 'mitigated': 2318, 'means': 2319, '1986)': 2320, 'cow/herd': 2321, 'glycerol': 2322, '(1999)': 2323, 'absorbed': 2324, 'consuming': 2325, 'well': 2326, 'fibrous': 2327, 'phenomenon': 2328, 'kg': 2329, 'protocol': 2330, 'way': 2331, 'homeorhesis1': 2332, 'silage-based': 2333, '1995;': 2334, 'gain': 2335, 'ingredient': 2336, 'adipose': 2337, 'ton': 2338, 'suitable': 2339, 'contributed': 2340, 'bulky': 2341, 'lipoproteins': 2342, 'caving': 2343, 'clinical': 2344, 'c)': 2345, 'nel': 2346, 'attention': 2347, 'play': 2348, 'ensuing': 2349, 'last': 2350, 'evidence': 2351, 'situations': 2352, 'round': 2353, 'dividing': 2354, 'assistance': 2355, 'lactate-utilizing': 2356, 'derailed': 2357, 'rapid': 2358, 'greatest': 2359, 'maintain': 2360, 'values': 2361, 'indicator': 2362, '2017)': 2363, 'acetone': 2364, 'progress': 2365, 'dietary': 2366, 'nutrients': 2367, 'contribute': 2368, 'species': 2369, '47': 2370, 'behind': 2371, 'intermittent': 2372, 'during': 2373, 'generate': 2374, 'suffer': 2375, 'period22': 2376, 'number': 2377, '1998);': 2378, 'player': 2379, 'avoid': 2380, 'environmental': 2381, '>04mmol/l': 2382, '6': 2383, 'glycol': 2384, '(ndf)': 2385, 'regular': 2386, 'mcal/kg': 2387, 'abomasa': 2388, 'attention8': 2389, '5': 2390, 'revealed': 2391, '(bell': 2392, '(5)': 2393, 'specific': 2394, 'balance': 2395, 'commingled': 2396, 'receiving': 2397, 'consequences': 2398, 'm]': 2399, 'content': 2400, 'central': 2401, 'butler': 2402, 'unfortunately': 2403, 'categories': 2404, 'genomic': 2405, 'buffers': 2406, '—': 2407, 'discuss': 2408, '1994;': 2409, 'exceeding': 2410, 'follows': 2411, 'properly': 2412, 'creates': 2413, 'jeopardizing': 2414, 'reducing': 2415, '(erdman': 2416, 'showing': 2417, '1979;': 2418, 'area': 2419, 'supplementation': 2420, 'tozer': 2421, 'summarises': 2422, 'reaching': 2423, 'monensin': 2424, 'profile': 2425, 'na': 2426, 'milking': 2427, 'however': 2428, 'al': 2429, 'mg/100': 2430, 'examine': 2431, 'andersen': 2432, 'contained': 2433, 'aspartate': 2434, 'despite': 2435, 'relationship': 2436, 'explain': 2437, 'mmol/l27': 2438, 'promote': 2439, 'causing': 2440, '1200': 2441, 'directly': 2442, 'implementing': 2443, 'wall': 2444, 'strong': 2445, 'rations': 2446, '10%': 2447, 'subjective': 2448, 'comparable': 2449, 'post-parturient': 2450, 'modifier': 2451, 'straw': 2452, '[ie': 2453, '40%13': 2454, 'propionate': 2455, 'salts': 2456, 'nefa': 2457, 'seem': 2458, 'appearing': 2459, '1995)': 2460, 'slow': 2461, 'dmi': 2462, 'bottles': 2463, 'analyzed': 2464, 'further': 2465, 'who': 2466, 'animals”': 2467, 'diagnosed': 2468, 'dry': 2469, 'producing': 2470, 'studied': 2471, 'every': 2472, 'lactation34': 2473, 'grouped': 2474, '120%': 2475, 'wk)': 2476, 'consumed': 2477, 'exponential': 2478, '14-week': 2479, 'xylazine)': 2480, 'need': 2481, 'recording': 2482, 'parameters': 2483, 'so': 2484, 'finding': 2485, 'very': 2486, 'cattle:': 2487, 'magnesium': 2488, 'suggested': 2489, 'degree': 2490, 'fed': 2491, 'parturition)': 2492, 'complete': 2493, 'cleaning': 2494, '(schultz': 2495, 'loss': 2496, 'dyk': 2497, 'homeostatic': 2498, 'understood': 2499, 'simultaneously': 2500, '(323': 2501, '1999;': 2502, '$396': 2503, 'calving;': 2504, 'fluctuate': 2505, '+': 2506, 'barley': 2507, 'foremost': 2508, 'dysfunction': 2509, 'mun': 2510, 'population': 2511, 'pertinent': 2512, 'these': 2513, 'diagnostic': 2514, 'prophylactically': 2515, 'average': 2516, 'moreover': 2517, 'oil': 2518, 'emerging': 2519, 'mobilization': 2520, 'important:': 2521, '(formigoni': 2522, 'a': 2523, 'challenge': 2524, 'element': 2525, '\\n': 2526, 'leukosis)': 2527, 'nearly': 2528, 'kg/d': 2529, 'programs': 2530, 'eq/l;': 2531, 'table': 2532, '360': 2533, 'make': 2534, 'voluntarily': 2535, 'principles': 2536, 'protein': 2537, '5-point': 2538, 'minimum': 2539, 'cows1': 2540, '(bhba)': 2541, 'wk': 2542, 'adjusting': 2543, 'typical': 2544, 'timely': 2545, 'monitored': 2546, 'implementation': 2547, 'physiology': 2548, 'hour': 2549, 'episode': 2550, '1983)': 2551, 'abstract\"': 2552, 'characteristic': 2553, 'periparturient': 2554, 'greater': 2555, 'nutrient': 2556, 'were': 2557, 'abomasums': 2558, 'abnormality': 2559, 'exhibiting': 2560, 'incidence': 2561, 'availability': 2562, 'dry-matter': 2563, '(raizman': 2564, 'maximum': 2565, 'acute': 2566, 'proposed': 2567, '(quigley': 2568, 'tremous': 2569, 'pre-fresh': 2570, 'otoxins': 2571, '34': 2572, 'refusals': 2573, 'cutoff': 2574, 'example': 2575, 'triacylglycerol': 2576, '19': 2577, '(vldl)': 2578, 'preventing': 2579, 'validate': 2580, '[dcad;': 2581, 'sorting': 2582, 'designs': 2583, 'extract': 2584, 'mammary': 2585, '(nel': 2586, '(gustafsson': 2587, 'adapt': 2588, 'result16’26': 2589, 'definitive': 2590, 'five': 2591, 'd': 2592, '605g/d': 2593, 'restrain': 2594, 'betahydroxybutyrate': 2595, '4kg': 2596, 'costs28': 2597, 'eg': 2598, 'unsuitable': 2599, 'underdiagnosed': 2600, 'inspection': 2601, 'excretion': 2602, 'deliveries': 2603, 'psps]': 2604, 'optimize': 2605, 'cc': 2606, '08m': 2607, 'chemotactic': 2608, 'manipulation': 2609, 'action': 2610, 'remains': 2611, 'great': 2612, 'adversely': 2613, 'proper': 2614, '(edwards': 2615, 'ruminating': 2616, 'biology': 2617, '3': 2618, 'rise': 2619, 'schultz': 2620, 'costs': 2621, '(hayirli': 2622, '×': 2623, '50:50': 2624, 'just': 2625, 'duffield': 2626, 'poor': 2627, 'ometritis': 2628, 'disturbance': 2629, 'gluconeogenesis': 2630, 'larger': 2631, 'arguably': 2632, 'infused': 2633, 'multifactorial': 2634, 'schemes': 2635, 'potential': 2636, 'comparison': 2637, 'feeding-': 2638, 'hence': 2639, 'reports': 2640, 'lipolysis': 2641, 'homeorhetic': 2642, 'lag': 2643, 'excellent': 2644, 'delivered': 2645, 'supplemented': 2646, 'questioned': 2647, 'factor': 2648, 'bacteria': 2649, 'nonesterified': 2650, 'issue': 2651, 'housed': 2652, 'frame': 2653, 'formation': 2654, 'antioxidant': 2655, 'likelihood': 2656, 'prone': 2657, '(given': 2658, 'in-line': 2659, 'carried': 2660, 'subject': 2661, '1988': 2662, 'dexamethasone': 2663, 'scenario': 2664, 'adaptive': 2665, 'dollars': 2666, 'hard': 2667, '[aiming': 2668, 'is': 2669, 'hypotheses': 2670, 'culled': 2671, 'epidemiology': 2672, 'laminitis': 2673, 'goals': 2674, 'via': 2675, 'understand': 2676, 'questionable': 2677, '110': 2678, 'overfeeding': 2679, 'veterinary': 2680, '(sauer': 2681, 'starch]': 2682, 'furthermore': 2683, 'hepatic': 2684, 'nerve': 2685, 's': 2686, 'adjusted': 2687, '(ferguson': 2688, 'different': 2689, 'changes': 2690, 'circumvent': 2691, 'eliminating': 2692, 'successful': 2693, 'shifted': 2694, 'conditional': 2695, 'needs': 2696, 'mmol/l;': 2697, 'nonketotic': 2698, 'prevalences': 2699, 'struggling': 2700, 'aim': 2701, 'ml': 2702, 'tissue': 2703, 'approached': 2704, 'usually': 2705, 'transition': 2706, 'as': 2707, '56': 2708, '(braun': 2709, 'drinking': 2710, 'tests': 2711, 'magnitude': 2712, '2004': 2713, 'meeting': 2714, '250': 2715, 'obesity': 2716, 'major': 2717, 'detoxification': 2718, '29': 2719, 'setala': 2720, 'hormone': 2721, 'positively': 2722, 'here': 2723, '2012)': 2724, '„': 2725, 'practical': 2726, 'observational': 2727, 'force-fed': 2728, 'overemphasized': 2729, 'condition;': 2730, 'reproductive': 2731, 'undesirable': 2732, 'ingredients': 2733, 'locking': 2734, '\"\\n': 2735, 'individual-animal': 2736, 'good': 2737, 'veterinarian': 2738, '2)': 2739, 'products': 2740, '42': 2741, 'addressing': 2742, 'anion': 2743, 'curtis': 2744, 'order': 2745, 'annual': 2746, 'metabolized': 2747, 'fat-corrected': 2748, 'green': 2749, 'located': 2750, '2003b)': 2751, 'service': 2752, 'grams': 2753, 'delaying': 2754, 'single': 2755, 'wheat': 2756, 'want': 2757, 'software': 2758, 'stopping': 2759, '(friggens': 2760, 'heavier': 2761, '(skaar': 2762, '(radostits': 2763, 'roughages': 2764, 'biological': 2765, '02': 2766, 'concentra¬': 2767, 'group': 2768, 'cycle': 2769, 'amount': 2770, '“transition”': 2771, 'she': 2772, 'maximization': 2773, '“modulating”': 2774, 'percent': 2775, '-': 2776, '(3)': 2777, 'inflammation': 2778, 'restricted-fed': 2779, 'comments': 2780, 'measured': 2781, 'dairy': 2782, 'unrealised': 2783, 'known': 2784, 'explored': 2785, 'lagging': 2786, 'shown': 2787, 'precalving': 2788, '>': 2789, 'waterers': 2790, 'parity': 2791, 'lactation2': 2792, '2014': 2793, 'inherently': 2794, 'repeated': 2795, 'ie': 2796, 'dramatically': 2797, 'indicated': 2798, 'excessively': 2799, 'flora': 2800, 'underscored': 2801, 'onset': 2802, 'injecting': 2803, 'exported': 2804, '1986;': 2805, 'lactation;': 2806, '(allen': 2807, 'each': 2808, 'post-partum15-36': 2809, 'leblanc': 2810, '1988)': 2811, 'goff': 2812, 'lb': 2813, '\"e\\n': 2814, '1000': 2815, 'by-product': 2816, 'supplements': 2817, 'grouping': 2818, 'vary': 2819, 'typically': 2820, 'meaningless': 2821, 'interpreted': 2822, 'cow’s': 2823, 'prevalent': 2824, 'whereas': 2825, 'begins': 2826, 'mention': 2827, 'cow': 2828, 'past': 2829, 'coordinated': 2830, 'mammals': 2831, 'try': 2832, 'alkalosis': 2833, 'affecting': 2834, 'proactively': 2835, 'pufa': 2836, 'main': 2837, 'there': 2838, 'phase;': 2839, 'only': 2840, 'transferring': 2841, 'herd': 2842, 'per': 2843, 'converts': 2844, 'proteolysis': 2845, '(1989)': 2846, 'alleys': 2847, 'mentioned': 2848, 'pre-': 2849, 'ext': 2850, 'hyperketonaemia': 2851, 'mmol/l': 2852, 'selected': 2853, 'limit': 2854, '(defined': 2855, '(roche': 2856, 'g)in': 2857, 'incorporated': 2858, 'sort': 2859, 'sufficiently': 2860, 'aptly': 2861, 'hay': 2862, 'philosophy': 2863, 'lactate': 2864, 'subclinically': 2865, 'noticed': 2866, '40': 2867, 'production-related': 2868, '350': 2869, 'susceptible': 2870, 'investigated': 2871, 'cows': 2872, 'conditions': 2873, 'standby': 2874, 'da': 2875, 'herd-level': 2876, '325': 2877, 'formulating': 2878, 'overfed': 2879, 'chemical': 2880, 'dufva': 2881, 'lactose': 2882, 'carbohydrates': 2883, 'recorded': 2884, 'left': 2885, 'gluten': 2886, 'task': 2887, 'form': 2888, '(ca)': 2889, 'parathyroid': 2890, 'incidences': 2891, 'trials': 2892, 'thin': 2893, '“there': 2894, 'fermentability': 2895, 'cyclicity': 2896, 'hypocalcaemia': 2897, 'elements': 2898, 'insemination': 2899, 'ketosis;': 2900, 'rayssiguier': 2901, 'environment': 2902, 'mixture': 2903, 'themselves': 2904, 'when': 2905, 'force': 2906, 'anti-inflammatory': 2907, 'you': 2908, '(simianer': 2909, 'adaptations': 2910, '(table': 2911, 'bauman': 2912, '6-week': 2913, 'sample': 2914, 'worse': 2915, 'alternative': 2916, 'discrepancy': 2917, '23': 2918, 'pooled': 2919, 'fat:': 2920, 'enable': 2921, 'prepartum': 2922, 'ts': 2923, 'pose': 2924, 'friday)': 2925, 'specificity)': 2926, '(an': 2927, 'note': 2928, 'framework': 2929, 'difficulty': 2930, 'g)': 2931, 'decisions': 2932, 'minimising': 2933, 'researchers': 2934, 'dann': 2935, '(martens': 2936, 'clinically': 2937, 'personnel': 2938, 'mj/kg)': 2939, 'offering': 2940, 'more': 2941, 'oetzel': 2942, 'assessment': 2943, 'function': 2944, 'inconsistent': 2945, '2011)': 2946, 'hormones': 2947, 'ponter': 2948, '535': 2949, 'adequately': 2950, 'few': 2951, 'available)': 2952, 'partum': 2953, 'needed': 2954, 'altered': 2955, 'groups': 2956, 'abnormally': 2957, 'remaining': 2958, 'offer': 2959, 'limit-fed': 2960, 'tried': 2961, 'growth': 2962, '70%': 2963, '1999)': 2964, 'wide': 2965, '(bw)': 2966, 'currie': 2967, 'above': 2968, 'nrc': 2969, 'context': 2970, 'food': 2971, 'eight': 2972, 'logically': 2973, '(hydrolyze': 2974, 'administer': 2975, 'unraveling': 2976, 'understanding': 2977, 'drug': 2978, 'address': 2979, 'histamine': 2980, 'culling': 2981, 'sequence': 2982, 'mertens': 2983, 'my': 2984, 'managers': 2985, '2003a;': 2986, 'the\"': 2987, 'with\"': 2988, 'determined': 2989, 'half': 2990, 'wet': 2991, 'injury': 2992, 'differing': 2993, '10-3)': 2994, 'north': 2995, 'housing': 2996, 'digestible': 2997, 'indicating': 2998, 'maintained': 2999, 'monitor': 3000, 'given': 3001, 'phenomena': 3002, 'discouraged': 3003, 'years': 3004, '(or': 3005, 'defined': 3006, 'pillars': 3007, '(geishauser': 3008, 'satisfactory': 3009, 'lower': 3010, 'estimated': 3011, '(sensitivity': 3012, 'cumulative': 3013, 'defining': 3014, 'parturition': 3015, 'consisted': 3016, 'provision': 3017, 'cases': 3018, 'document': 3019, '(dewhurst': 3020, 'over': 3021, '4)': 3022, '(see': 3023, 'speculation': 3024, 'urinary': 3025, '59%': 3026, 'remarkably': 3027, 'combination': 3028, 'utilize': 3029, '35%': 3030, '(rabelo': 3031, '(chilliard': 3032, 'bicarbonate': 3033, 'jones': 3034, 'gland2-11': 3035, 'inches': 3036, 'percentage': 3037, 'accumulating': 3038, 'objective': 3039, 'vldl': 3040, 'diseases': 3041, 'termed': 3042, 'reduction': 3043, 'starch': 3044, 'correction': 3045, 'key': 3046, 'noted': 3047, '(1013)': 3048, 'cortisol)': 3049, 'investigation': 3050, 'energy-related': 3051, 'umol/l': 3052, 'deps': 3053, 'infiltration': 3054, 'model(s)': 3055, 'score': 3056, 'strategy': 3057, 'snapshot': 3058, 'next': 3059, '328': 3060, 'white': 3061, 'including': 3062, '(nrc': 3063, 'because': 3064, '(fronk': 3065, 'arbitrarily': 3066, 'medication': 3067, 'previously': 3068, 'bovine': 3069, 'maintaining': 3070, 'storage': 3071, 'synthesis237': 3072, 'options': 3073, '3/4': 3074, 'things': 3075, 'in': 3076, 'prevented': 3077, 'characterized': 3078, 'acquired': 3079, '\"cattle\\n': 3080, 'price': 3081, '1998': 3082, 'interpret': 3083, 'investigate': 3084, '(specifically': 3085, 'disease': 3086, '1997)': 3087, '>07mmol/l': 3088, 'digesta': 3089, 'solution': 3090, 'involving': 3091, 'appetite-supressing': 3092, 'comfort': 3093, '1989;': 3094, 'rebound': 3095, 'slippery': 3096, 'research': 3097, 'sck': 3098, 'seeing': 3099, '(>40': 3100, 'traits': 3101, 'challenges:': 3102, 'neb': 3103, 'producers)': 3104, 'microarray': 3105, 'gluconate': 3106, 'dahl': 3107, 'harmful': 3108, '2007)': 3109, 'factor:': 3110, 'reliability': 3111, 'reynolds': 3112, 'believe': 3113, '(1993)': 3114, 'ensuring': 3115, 'front': 3116, 'vs': 3117, 'procedures—serum': 3118, 'success17': 3119, 'susceptibility': 3120, 'initiation': 3121, 'placenta)': 3122, '1998a)': 3123, 'still': 3124, 'numbers': 3125, 'rate': 3126, 'canadian': 3127, '55': 3128, 'discussed': 3129, 'prudent': 3130, 'ideal': 3131, 'while': 3132, 'processes': 3133, 'computer': 3134}\n"
     ]
    }
   ],
   "source": [
    "### label2Idx  word2Idx\n",
    "\n",
    "\n",
    "# sort the set to ensure '0' is assigned to 0\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "\n",
    "\n",
    "# create mapping for labels\n",
    "label2Idx = {}\n",
    "for label in sorted_labels:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "\n",
    "# create mapping for words\n",
    "word2Idx = {}\n",
    "if len(word2Idx) == 0:\n",
    "    word2Idx[\"PADDING_TOKEN\"] = len(word2Idx) # as 0\n",
    "    word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx) # as 1\n",
    "for word in wordSet:\n",
    "    word2Idx[word] = len(word2Idx)\n",
    "    \n",
    "# print some mapping \n",
    "\n",
    "print(\"idx2Label: \", len(idx2Label))\n",
    "print(\"idx2Label: \", idx2Label)\n",
    "print()\n",
    "print(\"word2Idx: \", len(word2Idx))\n",
    "print(\"word2Idx: \", word2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, Change words into representive index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sentences, train_labels:  [[1240, 2029, 2523, 2090, 410], [2683, 2838, 156, 1871, 1039, 1771, 2236, 2145, 2105, 605, 2373, 1747, 2124, 613, 2376, 2818, 2105, 294, 1170, 3076, 2706, 2828, 1693, 893, 1747, 1100, 2133, 1883, 1398, 1140, 2469, 2872, 1466, 1406, 2357, 1330, 1225, 2273, 2818, 2105, 294, 1693, 103, 2706, 2872], [2278, 2669, 2131, 1104, 2905, 2647, 506, 1747, 454, 3057, 2206, 1463, 1056, 1140, 1720, 2897, 802, 2838, 2669, 941], [2263, 2789, 126, 1140, 1366, 103, 1747, 1334, 415, 1560, 1140, 1257, 1699, 1015, 2523, 491, 2125, 2789, 567, 2852, 1747, 2093, 2669, 1140, 2198, 204, 1334, 802, 1941, 2618, 1140, 17, 1619, 2218, 333, 289], [3076, 2523, 1878, 258, 2278, 677, 2989, 802, 1747, 2875, 2561, 3076, 1084, 438, 2782, 1438, 677, 1514, 3030, 2273, 1374, 510, 1110, 560, 3076, 1438, 2273, 731, 1286, 2941, 2872, 1140, 665, 3076, 1438, 1369, 1493, 1695, 731, 2540, 2517, 2875, 2669, 2784, 2707, 579, 2124, 396, 1755, 2273, 2941, 1695, 2990, 103, 1467, 3018, 565, 2468, 758, 957, 531, 221, 3015, 2105, 319, 103, 1747, 1467, 3018, 1728, 758, 1747, 28, 359, 103, 2792, 136, 814, 1015, 2989, 802, 1747, 2516, 480, 2843, 2875, 172, 2669, 2941, 1695, 729, 2905, 402, 1771, 216, 289, 971, 3045, 3067, 1570, 434, 1380, 1409, 2105, 1785, 289, 1980, 434, 1348, 2496, 2496, 103, 1066, 432, 860, 2731, 1911, 475, 1030, 103, 1690, 1110, 1747, 1783, 1174, 1557, 1747, 3030, 2561, 1443, 1330, 1747, 2117, 361, 103, 867, 258, 2273, 1747, 3011, 17, 1972, 2782, 2872, 3076, 1084, 438, 2278, 2669, 2014, 1140, 1875, 802, 1747, 2746, 463, 1140, 1747, 2782, 1912, 1110, 2875, 954, 237, 1972, 2666]] \n",
      " [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0]]\n",
      "\n",
      "valid_sentences, valid_labels:  [[2905, 1537, 1127, 2513, 823, 1015, 451, 2693, 3076, 329, 2891, 103, 1099, 83, 282], [995, 1100, 2278, 2669, 1531, 485, 1140, 1041, 2956, 103, 2706, 2872, 2339, 1140, 1036, 3097], [1330, 643, 1747, 1005, 1517, 517, 3076, 3036, 2105, 2354, 802, 1330, 2377, 103, 2872, 3076, 1747, 1591, 1747, 517, 995, 1747, 1005, 1517, 802, 2808, 2828, 575, 223, 1140, 1466, 1871, 3011], [223, 1140, 307, 2154, 2669, 155, 2105, 579, 1332, 2419, 1414, 2872, 1466, 2044, 802, 2669, 1817, 1771, 1747, 1552, 103, 1747, 2469, 2828, 2419, 1483, 2710, 2154, 2669, 155, 1140, 1752, 1005, 1403], [1747, 353, 1771, 434, 1917, 2611, 1771, 1747, 808, 1865, 1284, 1868, 850, 2547, 3076, 1885]] [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "test_sentences, test_labels:  [[2905, 1537, 1127, 2513, 823, 1015, 451, 2693, 3076, 329, 2891, 103, 1099, 83, 282], [995, 1100, 2278, 2669, 1531, 485, 1140, 1041, 2956, 103, 2706, 2872, 2339, 1140, 1036, 3097], [1330, 643, 1747, 1005, 1517, 517, 3076, 3036, 2105, 2354, 802, 1330, 2377, 103, 2872, 3076, 1747, 1591, 1747, 517, 995, 1747, 1005, 1517, 802, 2808, 2828, 575, 223, 1140, 1466, 1871, 3011], [223, 1140, 307, 2154, 2669, 155, 2105, 579, 1332, 2419, 1414, 2872, 1466, 2044, 802, 2669, 1817, 1771, 1747, 1552, 103, 1747, 2469, 2828, 2419, 1483, 2710, 2154, 2669, 155, 1140, 1752, 1005, 1403], [1747, 353, 1771, 434, 1917, 2611, 1771, 1747, 808, 1865, 1284, 1868, 850, 2547, 3076, 1885]] [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def createMatrices(data, word2Idx, label2Idx):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    # get data \n",
    "    for split_labeled_text in data:\n",
    "        wordIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        # get word and data\n",
    "        for word, label in split_labeled_text:\n",
    "            \n",
    "            try:\n",
    "                # if is in the vocabulary\n",
    "                if word in word2Idx:\n",
    "                    wordIdx = word2Idx[word]\n",
    "\n",
    "                # if the lower case version is in the vocabulary\n",
    "                elif word.lower() in word2Idx:\n",
    "                    wordIdx = word2Idx[word.lower()] \n",
    "\n",
    "                # if not, assign to the unknown token \n",
    "                else:                \n",
    "                    wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "\n",
    "                # assign to the corrsponding index\n",
    "                wordIndices.append(wordIdx)\n",
    "\n",
    "                # fixing a bug of '' \n",
    "                if label != '':\n",
    "                    labelIndices.append(label2Idx[label])\n",
    "                    \n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "        # append the index to sentences\n",
    "        sentences.append(wordIndices)\n",
    "        labels.append(labelIndices)\n",
    "    return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = createMatrices(split_train, word2Idx, label2Idx)\n",
    "valid_sentences, valid_labels = createMatrices(split_valid, word2Idx, label2Idx)\n",
    "test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)\n",
    "\n",
    "# print to check \n",
    "\n",
    "print(\"train_sentences, train_labels: \", train_sentences[0:5],'\\n', train_labels[0:5])\n",
    "print()\n",
    "print(\"valid_sentences, valid_labels: \", valid_sentences[0:5], valid_labels[0:5])\n",
    "print()\n",
    "print(\"test_sentences, test_labels: \", test_sentences[0:5], test_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5, Pad the sentence into the same length\n",
    "\n",
    "### for fast computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "train_features, train_labels:  [[1240 2029 2523 2090  410    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]] [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "valid_features, valid_labels:  [[2905 1537 1127 2513  823 1015  451 2693 3076  329 2891  103 1099   83\n",
      "   282    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]] [[0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "test_features, test_labels:  [[2905 1537 1127 2513  823 1015  451 2693 3076  329 2891  103 1099   83\n",
      "   282    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]] [[0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "### padding with packages 'pad_sequences'\n",
    "\n",
    "# ========== set the max length ========== \n",
    "max_seq_len = 128\n",
    "\n",
    "def padding(sentences, labels, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(sentences, max_len,       \n",
    "    padding='post')\n",
    "    padded_labels = pad_sequences(labels, max_len, padding='post')\n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "train_features, train_labels = padding(train_sentences, train_labels, max_seq_len, padding='post' )\n",
    "valid_features, valid_labels = padding(valid_sentences, valid_labels, max_seq_len, padding='post' )\n",
    "test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post' )\n",
    "\n",
    "# check for the results \n",
    "print(len(train_features[1]) == len(train_features[2]))\n",
    "print()\n",
    "print(\"train_features, train_labels: \", train_features[0:1], train_labels[0:1])\n",
    "print()\n",
    "print(\"valid_features, valid_labels: \", valid_features[0:1], valid_labels[0:1])\n",
    "print()\n",
    "print(\"test_features, test_labels: \", test_features[0:1], test_labels[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, Using the Global Vector Word Embedding\n",
    "\n",
    "### Glove is a vector for word embedding, the machine understand the words by the location of them in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set the dimension for each word \n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# loading glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open('embeddings/glove.6B.100d.txt', encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.strip().split(' ')\n",
    "    word = values[0] # the first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') #100d vectors  representing the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embedding_matrix = np.zeros((len(word2Idx), EMBEDDING_DIM))\n",
    "\n",
    "# word embeddings for the tokens\n",
    "for word,i in word2Idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Training with tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, Batching and shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched_train_dataset:  <BatchDataset shapes: ((64, 128), (64, 128)), types: (tf.int32, tf.int32)>\n",
      "batched_valid_dataset:  <BatchDataset shapes: ((64, 128), (64, 128)), types: (tf.int32, tf.int32)>\n",
      "batched_test_dataset:  <BatchDataset shapes: ((64, 128), (64, 128)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "### using tf.data.Dataset.from_tensor_slices \n",
    "\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "train_batch_size = 64\n",
    "valid_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_features, valid_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "# shuffling the training dataset \n",
    "shuffled_train_dataset = train_dataset.shuffle(buffer_size=train_features.shape[0], reshuffle_each_iteration=True)\n",
    "\n",
    "# batching the three datasets\n",
    "batched_train_dataset = shuffled_train_dataset.batch(train_batch_size, drop_remainder=True) # drop_remiainder: ignore the last batch if nesscery\n",
    "batched_valid_dataset = valid_dataset.batch(valid_batch_size, drop_remainder=True)\n",
    "batched_test_dataset = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
    "\n",
    "# checking \n",
    "print(\"batched_train_dataset: \", batched_train_dataset)\n",
    "print(\"batched_valid_dataset: \", batched_valid_dataset)\n",
    "print(\"batched_test_dataset: \", batched_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, Bi-direction Long-Short-Term Memory Neural Network (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===== model1 bi-lstm128\n",
    "\n",
    "class TFNer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, num_labels, weights):\n",
    "        super(TFNer, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = layers.Embedding(input_dim=embed_input_dim, \n",
    "                                          output_dim=embed_output_dim, \n",
    "                                          weights=weights, \n",
    "                                          input_length=max_seq_len, \n",
    "                                          trainable=False, \n",
    "                                          mask_zero=True)\n",
    "        \n",
    "        # Bidrectional layer\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))\n",
    "        \n",
    "        # Dense layer\n",
    "        self.dense = layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs) # batchsize, max_seq_len, embedding_output_dim\n",
    "        x = self.bilstm(x) # batchsize, max_seq_len, hidden_dim_bilstm\n",
    "        logits = self.dense(x) # batchsize, max_seq_len, num_labels\n",
    "        \n",
    "        # return a logist score\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, Define the optimizer: Adam \n",
    "### and losses: SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "# model\n",
    "model = TFNer(max_seq_len=max_seq_len,embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# losses: scce\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastprogress\n",
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os\n",
    "import os\n",
    "\n",
    "# math\n",
    "import math\n",
    "\n",
    "# pickle: serializes objects so they can be saved to a file, \n",
    "# and loaded in a program again later on\n",
    "import pickle\n",
    "\n",
    "# logging: use 'logger' to log messages \n",
    "# have 5 levels: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "import logging\n",
    "\n",
    "# to create the the command line interface\n",
    "import argparse\n",
    "\n",
    "# itertoole\n",
    "import itertools\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from model import TFNer\n",
    "\n",
    "# pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# master_bar, progress_bar \n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# from preprocess import split_text_label, padding, createMatrices\n",
    "\n",
    "# seqeval: for sequence labeling \n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for model\n",
    "num_labels = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "# model\n",
    "model1 = TFNer(max_seq_len=max_seq_len,embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# losses: scce\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 7s 353ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.7827 - val_loss: 0.0474 - val_sparse_categorical_accuracy: 0.9349\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 3s 245ms/step - loss: 0.0484 - sparse_categorical_accuracy: 0.9328 - val_loss: 0.0358 - val_sparse_categorical_accuracy: 0.9459\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 4s 257ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.0316 - val_sparse_categorical_accuracy: 0.9525\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 4s 264ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9531 - val_loss: 0.0260 - val_sparse_categorical_accuracy: 0.9610\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 4s 263ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9671 - val_loss: 0.0240 - val_sparse_categorical_accuracy: 0.9644\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 4s 267ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9759 - val_loss: 0.0223 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 4s 269ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9786 - val_loss: 0.0220 - val_sparse_categorical_accuracy: 0.9693\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 4s 261ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.0221 - val_sparse_categorical_accuracy: 0.9713\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 4s 279ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9836 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 0.9739\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9866 - val_loss: 0.0204 - val_sparse_categorical_accuracy: 0.9725\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 4s 276ms/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0220 - val_sparse_categorical_accuracy: 0.9735\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9921 - val_loss: 0.0207 - val_sparse_categorical_accuracy: 0.9735\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9959 - val_loss: 0.0234 - val_sparse_categorical_accuracy: 0.9699\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.0037 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.0233 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 4s 268ms/step - loss: 0.0030 - sparse_categorical_accuracy: 0.9972 - val_loss: 0.0246 - val_sparse_categorical_accuracy: 0.9735\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0253 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 4s 277ms/step - loss: 0.0021 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0244 - val_sparse_categorical_accuracy: 0.9733\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 4s 270ms/step - loss: 0.0011 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0272 - val_sparse_categorical_accuracy: 0.9727\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 4s 266ms/step - loss: 9.9296e-04 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.0278 - val_sparse_categorical_accuracy: 0.9733\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 4s 274ms/step - loss: 6.1894e-04 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.0292 - val_sparse_categorical_accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "### ===the second running with large data \n",
    "\n",
    "# ===== model1 \n",
    "model1.compile(loss = scce, optimizer = optimizer, metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# fit \n",
    "history2 = model1.fit(batched_train_dataset, \n",
    "                    epochs = 20, \n",
    "                    validation_data = batched_valid_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 5s 444ms/step - loss: 0.1598 - sparse_categorical_accuracy: 0.7149 - val_loss: 0.0548 - val_sparse_categorical_accuracy: 0.9275\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9246 - val_loss: 0.0475 - val_sparse_categorical_accuracy: 0.9364\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0487 - sparse_categorical_accuracy: 0.9302 - val_loss: 0.0421 - val_sparse_categorical_accuracy: 0.9402\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.0380 - val_sparse_categorical_accuracy: 0.9444\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 2s 270ms/step - loss: 0.0357 - sparse_categorical_accuracy: 0.9437 - val_loss: 0.0337 - val_sparse_categorical_accuracy: 0.9487\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 2s 259ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9504 - val_loss: 0.0302 - val_sparse_categorical_accuracy: 0.9554\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9602 - val_loss: 0.0294 - val_sparse_categorical_accuracy: 0.9566\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.0299 - val_sparse_categorical_accuracy: 0.9598\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9687 - val_loss: 0.0286 - val_sparse_categorical_accuracy: 0.9620\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 2s 280ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9767 - val_loss: 0.0267 - val_sparse_categorical_accuracy: 0.9618\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 2s 270ms/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9792 - val_loss: 0.0256 - val_sparse_categorical_accuracy: 0.9644\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0257 - val_sparse_categorical_accuracy: 0.9628\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.0260 - val_sparse_categorical_accuracy: 0.9640\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0286 - val_sparse_categorical_accuracy: 0.9665\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9863 - val_loss: 0.0288 - val_sparse_categorical_accuracy: 0.9671\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9891 - val_loss: 0.0266 - val_sparse_categorical_accuracy: 0.9661\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0263 - val_sparse_categorical_accuracy: 0.9651\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9941 - val_loss: 0.0279 - val_sparse_categorical_accuracy: 0.9644\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 2s 273ms/step - loss: 0.0037 - sparse_categorical_accuracy: 0.9968 - val_loss: 0.0284 - val_sparse_categorical_accuracy: 0.9634\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 2s 267ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9976 - val_loss: 0.0303 - val_sparse_categorical_accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "### ===first run with the whole training data\n",
    "\n",
    "# ===== model1 \n",
    "model1.compile(loss = scce, optimizer = optimizer, metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# fit \n",
    "history1 = model1.fit(batched_train_dataset, \n",
    "                    epochs = 20, \n",
    "                    validation_data = batched_valid_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot history: MAE\n",
    "\n",
    "plt.plot(history1.history['val_loss'], label='Small Dataset', color = 'red', linestyle = 'dotted')\n",
    "\n",
    "plt.plot(history2.history['val_loss'], label='Large Dataset', color = 'green', linestyle = 'dotted')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Different Training Data Size')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.xlabel('No.epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, Set the logging and progress bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fastprogress\n",
    "\n",
    "# ========== num of epochs ==========\n",
    "epochs = 20\n",
    "\n",
    "# train_pd_max_len, valid_pb_max_len, test_pb_max_len\n",
    "epoch_bar = master_bar(range(epochs))\n",
    "train_pb_max_len = math.ceil(float(len(train_features))/float(train_batch_size))\n",
    "valid_pb_max_len = math.ceil(float(len(valid_features))/float(valid_batch_size))\n",
    "test_pb_max_len = math.ceil(float(len(test_features))/float(test_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tf.summary.create_file_writer: a trianing log \n",
    "\n",
    "train_log_dir = f\"/logs\"\n",
    "valid_log_dir = f\"/logs\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, Custom training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loss\n",
    "train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "\n",
    "# validing loss\n",
    "valid_loss_metric = tf.keras.metrics.Mean('valid_loss', dtype=tf.float32)\n",
    "\n",
    "# customize training loop\n",
    "def train_step_fn(sentences_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(sentences_batch) # batchsize, max_seq_len, num_labels\n",
    "        loss = scce(labels_batch, logits) # batchsize,max_seq_len\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "    return loss, logits\n",
    "\n",
    "# customize validing loop \n",
    "def valid_step_fn(sentences_batch, labels_batch):\n",
    "    logits = model(sentences_batch)\n",
    "    loss = scce(labels_batch, logits)\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    \n",
    "    # customize training epoch bar \n",
    "    with train_summary_writer.as_default():\n",
    "        for sentences_batch, labels_batch in progress_bar(\n",
    "            batched_train_dataset, \n",
    "            total=train_pb_max_len, \n",
    "            parent=epoch_bar):\n",
    "\n",
    "            loss, logits = train_step_fn(sentences_batch, labels_batch)\n",
    "            \n",
    "            train_loss_metric(loss)\n",
    "            \n",
    "            epoch_bar.child.comment = f'training loss : {train_loss_metric.result()}'\n",
    "        \n",
    "        tf.summary.scalar('training loss', train_loss_metric.result(), step=epoch)\n",
    "        train_loss_metric.reset_states()\n",
    "    \n",
    "    # customize validing epoch bar \n",
    "    with valid_summary_writer.as_default():\n",
    "        for sentences_batch, labels_batch in progress_bar(\n",
    "            batched_valid_dataset, \n",
    "            total=valid_pb_max_len, \n",
    "            parent=epoch_bar):\n",
    "            \n",
    "            loss, logits = valid_step_fn(sentences_batch, labels_batch)\n",
    "            \n",
    "            valid_loss_metric.update_state(loss)\n",
    "\n",
    "            epoch_bar.child.comment = f'validation loss : {valid_loss_metric.result()}'\n",
    "\n",
    "        # logging after each epoch !\n",
    "        tf.summary.scalar('valid loss', valid_loss_metric.result(), step=epoch)\n",
    "        valid_loss_metric.reset_states()\n",
    "\n",
    "model.save_weights(f\"./model_weights\",save_format='tf')  \n",
    "# logger.info(f\"Model weights saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4, Evaluating model performance on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, two functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### assgining the label to each word \n",
    "\n",
    "def idx_to_label(predictions, correct, idx2Label):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        predictions: predicitons of the model (idx)\n",
    "        correct: targets from the text (idx)\n",
    "    output: \n",
    "        label_correct: predicitons of the model (label)\n",
    "        label_pred: targets from the text (label)\n",
    "    \"\"\"\n",
    "    label_pred = []    \n",
    "    for sentence in predictions:\n",
    "        for i in sentence:\n",
    "            label_pred.append([idx2Label[elem] for elem in i ]) \n",
    "\n",
    "    label_correct = []  \n",
    "    if correct != None:\n",
    "        for sentence in correct:\n",
    "            for i in sentence:\n",
    "                label_correct.append([idx2Label[elem] for elem in i ]) \n",
    "        \n",
    "    return label_correct, label_pred\n",
    "\n",
    "### predict each sentence: using pad_sequences\n",
    "\n",
    "def predict_single_sentence(sentence, word2Idx, max_seq_len):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        sentence: string\n",
    "        word2Idx: function\n",
    "        max_seq_len: int\n",
    "    output: \n",
    "        length: int\n",
    "        masks: array of 1 and 0\n",
    "        padded_inputs: array of padded index for the sentence \n",
    "    \"\"\"\n",
    "    sentence = list(sentence.split(\" \"))\n",
    "    sentences = []\n",
    "    wordIndices = []\n",
    "    masks = []\n",
    "    length = len(sentence)\n",
    "\n",
    "    # assining index to the sentence\n",
    "    for word in sentence:\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]                 \n",
    "        else:                \n",
    "            wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "        wordIndices.append(wordIdx)\n",
    "\n",
    "    # len of mask = len of sentence \n",
    "    maskindices = [1]*len(wordIndices)\n",
    "\n",
    "    # append\n",
    "    sentences.append(wordIndices)\n",
    "    masks.append(maskindices)\n",
    "\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sentences, \n",
    "        maxlen=max_seq_len, \n",
    "        padding=\"post\")\n",
    "\n",
    "    masks = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        masks, \n",
    "        maxlen=max_seq_len, \n",
    "        padding=\"post\")\n",
    "\n",
    "    return length, masks, padded_inputs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x25eb5ce8e80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_batch_size = 64\n",
    "\n",
    "# padding sentences and labels to max_length of 128\n",
    "max_seq_len = 128\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "\n",
    "# idx2Label = pickle.load(open(os.path.join(\"idx2Label.pkl\"), 'rb'))\n",
    "label2Idx = {v:k for k,v in idx2Label.items()}\n",
    "num_labels = len(label2Idx)\n",
    "\n",
    "\n",
    "# word2Idx = pickle.load(open(os.path.join(\"word2Idx.pkl\"), 'rb'))\n",
    "# Embedding_matrix = pickle.load(open(os.path.join(\"embedding.pkl\"), 'rb'))\n",
    "# logger.info(\"Loaded idx2Label, word2Idx and Embedding matrix pickle files\")\n",
    "\n",
    "# loading the model\n",
    "testmodel =  TFNer(\n",
    "    max_seq_len=max_seq_len, \n",
    "    embed_input_dim=len(word2Idx), \n",
    "    embed_output_dim=EMBEDDING_DIM, \n",
    "    weights=[embedding_matrix], \n",
    "    num_labels=num_labels)\n",
    "\n",
    "# testmodel.load_weights(f\"{args.model_dir}/model_weights\")\n",
    "testmodel.load_weights(\"./model_weights\")\n",
    "# logger.info(\"Model weights restored\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, Evaluation on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<BatchDataset shapes: ((64, 128), (64, 128)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "### processing the test set \n",
    "\n",
    "# extract from the txt\n",
    "# split_test = split_text_label(\"test.txt\")\n",
    "\n",
    "# indexing \n",
    "test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)\n",
    "\n",
    "# padding\n",
    "test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post' )\n",
    "\n",
    "# logger\n",
    "# logger.info(f\"Test features shape is {test_features.shape} and labels shape is{test_labels.shape}\")\n",
    "\n",
    "# input to tesorflow\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "# batching\n",
    "batched_test_dataset = test_dataset.batch(test_batch_size, drop_remainder=True)\n",
    "\n",
    "###  checking \n",
    "\n",
    "# print(test_features[0:5], test_labels[0:5])\n",
    "print()\n",
    "print(batched_test_dataset)\n",
    "\n",
    "# print(test_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DIS       0.80      0.71      0.75        34\n",
      "         NUT       0.96      0.89      0.93        57\n",
      "\n",
      "   micro avg       0.90      0.82      0.86        91\n",
      "   macro avg       0.88      0.80      0.84        91\n",
      "weighted avg       0.90      0.82      0.86        91\n",
      "\n",
      "\n",
      "The length of label_pred:  64\n"
     ]
    }
   ],
   "source": [
    "### output the test set evaluation results \n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "\n",
    "### epoch_bar = master_bar(range(epochs))\n",
    "\n",
    "test_pb_max_len = math.ceil(float(len(test_features))/float(test_batch_size))\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "#for sentences_batch, labels_batch in progress_bar(batched_test_dataset, total=test_pb_max_len):\n",
    "\n",
    "logits = testmodel(sentences_batch)\n",
    "temp1 = tf.nn.softmax(logits)   \n",
    "preds = tf.argmax(temp1, axis=2)\n",
    "true_labels.append(np.asarray(labels_batch))\n",
    "pred_labels.append(np.asarray(preds))\n",
    "\n",
    "label_correct, label_pred = idx_to_label(pred_labels, true_labels, idx2Label)\n",
    "\n",
    "#print(preds)\n",
    "#print(pred_labels, true_labels)\n",
    "#print(label_correct, label_pred)\n",
    "# f1_score(pred_labels, true_labels)\n",
    "print(classification_report(label_correct, label_pred))\n",
    "#logger.info(f\"\\nResults for the test dataset\") \n",
    "# logger.info(f\"\\n{report}\")\n",
    "\n",
    "print()\n",
    "# print(label_pred[0:5])\n",
    "print(\"The length of label_pred: \", len(label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# ! pip install nltk \n",
    "# nltk.download()\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== load in the prediction text ==========\n",
    "### tokenizing the prediction dataset as sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prediction.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-f903a3dbde3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# ========== load in the prediction text ==========\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprediction_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prediction.txt'"
     ]
    }
   ],
   "source": [
    "### tokenizing the prediction dataset \n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# ========== load in the prediction text ==========\n",
    "with open(\"prediction.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    prediction_corpus = f.readlines()\n",
    "\n",
    "# print(training_corpus[0:5])\n",
    "# print(len(training_corpus))\n",
    "# print()\n",
    "\n",
    "\n",
    "# remove some specical objects \n",
    "\n",
    "import re\n",
    "\n",
    "prediction_corpus = str(prediction_corpus)\n",
    "prediction_corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", prediction_corpus)\n",
    "prediction_corpus = prediction_corpus.lower()\n",
    "\n",
    "prediction_sentences = sent_tokenize(prediction_corpus)\n",
    "prediction_words = word_tokenize(prediction_corpus)\n",
    "\n",
    "# check and see \n",
    "print(\"prediction_sentences: \", prediction_sentences)\n",
    "print(\"prediction_words: \", prediction_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making prediction on the prediction dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make prediction on the prediction dataset \n",
    "\n",
    "# test_sentence = \"Clinical CM is always associated with a drop in the dry matter intake of the cows in transition period\"\n",
    "\n",
    "def predict_text(test_sentence):\n",
    "    \n",
    "    length, masks, padded_inputs = predict_single_sentence(test_sentence, word2Idx, max_seq_len)\n",
    "    padded_inputs = tf.expand_dims(padded_inputs, 0)\n",
    "\n",
    "    true_labels = None\n",
    "    pred_labels = []\n",
    "    pred_logits = []\n",
    "\n",
    "    for sentence in padded_inputs:\n",
    "        logits = testmodel(sentence)\n",
    "        temp1 = tf.nn.softmax(logits) \n",
    "        max_values = tf.reduce_max(temp1,axis=-1)\n",
    "\n",
    "        masked_max_values = max_values * masks \n",
    "        preds = tf.argmax(temp1, axis=2)\n",
    "        pred_labels.append(np.asarray(preds))\n",
    "        pred_logits.extend(np.asarray(masked_max_values))\n",
    "\n",
    "    _,label_pred  = idx_to_label(pred_labels, true_labels, idx2Label)\n",
    "\n",
    "    # logger.info(f\"Results for - \\\"{test_sentence}\\\"\")\n",
    "\n",
    "    label_pred = label_pred[0][:length] \n",
    "    pred_logits = pred_logits[0][:length]\n",
    "    # logger.info(f\"Labels predicted are {label_pred}\")\n",
    "    # logger.info(f\"with a confidence of {pred_logits}\")\n",
    "    \n",
    "    return label_pred\n",
    "\n",
    "label_preds = []\n",
    "for sentence in prediction_sentences: \n",
    "    label_pred = predict_text(sentence)\n",
    "    label_preds.append(label_pred)\n",
    "    \n",
    "print(\"label_preds: \", label_preds[0:5])\n",
    "\n",
    "# print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of prediction_labels:  32\n",
      "\n",
      "\n",
      "The length of prediction_sentences:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-fe793a7509f9>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  prediction_labels = np.array(label_preds)\n"
     ]
    }
   ],
   "source": [
    "### check for the length \n",
    "\n",
    "prediction_labels = np.array(label_preds) \n",
    "\n",
    "prediction_labels = prediction_labels.flatten()\n",
    "\n",
    "# check and print\n",
    "print(\"The length of prediction_labels: \", len(prediction_labels))\n",
    "print()\n",
    "print(\"The length of prediction_sentences: \", len(prediction_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== name of the prediction output excel =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "### output to an excel \n",
    "\n",
    "# as words, labels and pos\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "df = pd.DataFrame([prediction_sentences,prediction_labels])\n",
    "\n",
    "# transpose the columns and rows\n",
    "d_f = df.T\n",
    "\n",
    "# ========== name of the prediction output excel ==========\n",
    "d_f.to_excel('Predictions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5, Prediction for a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      "['periparturient', 'diseases', 'such', 'as', 'milk', 'fever', 'ketosis', 'and', 'displaced', 'abomasums', 'can', 'all', 'be', 'associated', 'with', 'poor', 'transition']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DIS', 'I-DIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"\"\"Periparturient diseases such as milk fever, ketosis, and displaced abomasums can all be associated with poor transition\"\"\"\n",
    "\n",
    "length, masks, padded_inputs = predict_single_sentence(test_sentence, word2Idx, max_seq_len)\n",
    "padded_inputs = tf.expand_dims(padded_inputs, 0)\n",
    "\n",
    "true_labels = None\n",
    "pred_labels = []\n",
    "pred_logits = []\n",
    "\n",
    "for sentence in padded_inputs:\n",
    "    logits = testmodel(sentence)\n",
    "    temp1 = tf.nn.softmax(logits) \n",
    "    max_values = tf.reduce_max(temp1,axis=-1)\n",
    "\n",
    "    masked_max_values = max_values * masks \n",
    "    preds = tf.argmax(temp1, axis=2)\n",
    "    pred_labels.append(np.asarray(preds))\n",
    "    pred_logits.extend(np.asarray(masked_max_values))\n",
    "_,label_pred  = idx_to_label(pred_labels, true_labels, idx2Label)\n",
    "\n",
    "\n",
    "\n",
    "label_pred = label_pred[0][:length] \n",
    "pred_logits = pred_logits[0][:length]\n",
    "\n",
    "\n",
    "\n",
    "### the splited sentences: words \n",
    "\n",
    "\n",
    "import re\n",
    "from nltk import word_tokenize \n",
    "\n",
    "# remove punct \n",
    "words = str(test_sentence)\n",
    "words = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", words)\n",
    "words = words.lower()\n",
    "\n",
    "# tokenize\n",
    "words = word_tokenize(words)\n",
    "\n",
    "# check for length \n",
    "print(len(words) == len(label_pred))\n",
    "print()\n",
    "print(words)\n",
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_word:  ['displaced', 'abomasums']\n",
      "\n",
      "ner_label:  ['B-DIS', 'I-DIS']\n"
     ]
    }
   ],
   "source": [
    "### a pretter output \n",
    "\n",
    "ner_word = []\n",
    "ner_label = []\n",
    "for i, j in zip(words, label_pred): \n",
    "    if j != 'O':\n",
    "        ner_word.append(i)\n",
    "        ner_label.append(j)\n",
    "        \n",
    "# check and see \n",
    "print(\"ner_word: \", ner_word) \n",
    "print()\n",
    "print(\"ner_label: \", ner_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
