{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary_Tagger2: This file is used to tag the agumentated data\n",
    "\n",
    "# 1, Load text and dictionary: new_tokenize_text, nutrition_dict, disease_dict\n",
    "\n",
    "# 2, Expand each sentence: expanded_word_list\n",
    "\n",
    "# 3, Get the list of entity: ent_list_dis, ent_list_nut\n",
    "\n",
    "# 4, Tag the origin sentence list: \n",
    "\n",
    "# t_sent_split(origin list), \n",
    "\n",
    "# dis_tagged_sent(tagged by dis),\n",
    "\n",
    "# nut_tagged_sent,\n",
    "\n",
    "# 5, Concat two tagged list: \n",
    "\n",
    "# tag_sum_mid,\n",
    "\n",
    "# 6, Concat tagged lists with 'END', origin lists with 'END'\n",
    "\n",
    "# final_tag\n",
    "\n",
    "# ori_sum_mid\n",
    "\n",
    "# 7, Output to excel \n",
    "\n",
    "# rich_sentence.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a host ofs\n",
      "157369\n"
     ]
    }
   ],
   "source": [
    "# ========== Raw_Corpus.txt ==========\n",
    "with open(\"training_set_txt_12_20.txt\", 'r', encoding = 'utf-8') as f:\n",
    "    training_corpus = f.read()\n",
    "\n",
    "# print(training_corpus[0:5])\n",
    "# print(len(training_corpus))\n",
    "# print()\n",
    "\n",
    "\n",
    "# remove some specical objects \n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "training_corpus = str(training_corpus)\n",
    "training_corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", training_corpus)\n",
    "training_corpus = training_corpus.lower()\n",
    "print(training_corpus[0:10])\n",
    "print(len(training_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "group cow relocations to only take place once a week.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize \n",
    "\n",
    "tokenize_text = sent_tokenize(training_corpus)\n",
    "\n",
    "# print(len(tokenize_text))\n",
    "# print(tokenize_text[90:100])\n",
    "\n",
    "# lower \n",
    "new_tokenize_text = []\n",
    "for i in tokenize_text: \n",
    "    i = i.lower()\n",
    "    new_tokenize_text.append(i)\n",
    "    \n",
    "print(len(new_tokenize_text))\n",
    "print(new_tokenize_text[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== Dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acetate acid', 'acetoacetic acid', 'acetyl coenzyme', 'additives', 'alfalfa', 'alternate grazing', 'amino acid', 'amino acids', 'amylase', 'antibiotic']\n",
      "234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_NUT_2021_12_20.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "nutrition_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(nutrition_dict[0:10])\n",
    "print(len(nutrition_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abomasal displacement', 'abortion', 'acetonaemia', 'acorn toxicity', 'actinobacillosis', 'actinomycosis', 'acute rumen acidosis', 'ammoniated feeds', 'anoestrus', 'anthrax']\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel (r'Dict_DIS_2021_12_20.xlsx') \n",
    "df = pd.DataFrame(data, columns= ['Name'])\n",
    "# print (df)\n",
    "\n",
    "disease_dict  = df['Name'].values.tolist()\n",
    "\n",
    "print(disease_dict[0:10])\n",
    "print(len(disease_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== expand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to get two and three combinations of entity words \n",
    "\n",
    "import re \n",
    "\n",
    "def get_two_three_comb(sentence):\n",
    "    \n",
    "        # remove punct \n",
    "    str1 = sentence.replace(',', '')\n",
    "    str1 = str1.replace(' .', '')\n",
    "    \n",
    "    # split sentence\n",
    "    str1 = str1.split(' ')\n",
    "    \n",
    "    # for comb of two \n",
    "    comb_12 = []\n",
    "    for i, j in enumerate(str1):\n",
    "\n",
    "        comb12 = str1[i - 1] + \" \" + str1[i]\n",
    "        comb_12.append(comb12)\n",
    "        \n",
    "    # for comb of three \n",
    "    comb_13 = []\n",
    "    str1 = str1 + ['O']\n",
    "    comb_12 = ['O'] + comb_12\n",
    "    for i, j in zip(str1, comb_12): \n",
    "        comb13 = j + \" \"+ i\n",
    "        comb_13.append(comb13)  \n",
    "    \n",
    "    # final list \n",
    "    comb123 = str1 + comb_12 + comb_13\n",
    "    \n",
    "    return comb123\n",
    "\n",
    "# print(get_two_three_comb(words_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week.', 'O', 'O', 'week. group', 'group cow', 'cow relocations', 'relocations to', 'to only', 'only take', 'take place', 'place once', 'once a', 'a week.', 'O group', 'week. group cow', 'group cow relocations', 'cow relocations to', 'relocations to only', 'to only take', 'only take place', 'take place once', 'place once a', 'once a week.', 'a week. O']\n"
     ]
    }
   ],
   "source": [
    "expanded_word_list = []\n",
    "\n",
    "for i, j in enumerate(new_tokenize_text):\n",
    "    temp = get_two_three_comb(new_tokenize_text[i])\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    expanded_word_list.append(temp)\n",
    "    \n",
    "        \n",
    "# print(parsed_entities[i]['text'])\n",
    "\n",
    "print(len(expanded_word_list))\n",
    "print()\n",
    "print(expanded_word_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== get entity list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_for_a_sent(sent, dict1):\n",
    "    tagged_sent = []\n",
    "    for word in sent:\n",
    "        if word in dict1:\n",
    "            tagged_sent.append(word)\n",
    "            tagged_sent = list(tagged_sent)\n",
    "    return tagged_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ent_list_dis = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], disease_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_dis.append(temp1)\n",
    "    \n",
    "print(len(ent_list_dis))\n",
    "print(ent_list_dis[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for nutrition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ent_list_nut = []\n",
    "\n",
    "for i, j in enumerate(expanded_word_list):\n",
    "    temp1 = tag_for_a_sent(expanded_word_list[i], nutrition_dict)\n",
    "    # lower \n",
    "    # str_temp = str(temp).lower()\n",
    "    ent_list_nut.append(temp1)\n",
    "    \n",
    "print(len(ent_list_nut))\n",
    "print(ent_list_nut[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===== tag the origin list ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==== there is a change to delete '' in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        if i2 != '':\n",
    "            t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tag_dis(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-DIS'\n",
    "                    t_sent_split[i+1] = 'I-DIS'\n",
    "                    t_sent_split[i+2] = 'I-DIS'\n",
    "            \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "dis_tagged_sent = []\n",
    "\n",
    "for i, j in zip(t_sent_split, ent_list_dis):\n",
    "    temp2 = tag_dis(i,j)\n",
    "    dis_tagged_sent.append(temp2)\n",
    "    \n",
    "print(len(dis_tagged_sent))\n",
    "print(dis_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_nut(t_sent_split, t_list):\n",
    "    \n",
    "    for i, j in enumerate(t_sent_split):\n",
    "        try:\n",
    "            # for the first entity in the t_list\n",
    "            if j == t_list[0].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[0].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[0].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[0].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the second entity in the t_list\n",
    "            if j == t_list[1].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[1].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[1].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[1].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the third entity in the t_list\n",
    "            if j == t_list[2].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[2].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[2].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[2].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fourth entity in the t_list\n",
    "            if j == t_list[3].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[3].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[3].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[3].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'           \n",
    "\n",
    "            # for the fifth entity in the t_list\n",
    "            if j == t_list[4].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[4].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[4].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[4].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'                    \n",
    "\n",
    "            # for the sixth entity in the t_list\n",
    "            if j == t_list[5].split(' ')[0]:\n",
    "\n",
    "                # if is a single word\n",
    "                if len(t_list[5].split(' ')) == 1:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "\n",
    "                # if is a len two word entity\n",
    "                if len(t_list[5].split(' ')) == 2:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "\n",
    "                # if is a len three word entity \n",
    "                if len(t_list[5].split(' ')) == 3:\n",
    "                    t_sent_split[i] = 'B-NUT'\n",
    "                    t_sent_split[i+1] = 'I-NUT'\n",
    "                    t_sent_split[i+2] = 'I-NUT'\n",
    "\n",
    "        except IndexError:\n",
    "            pass   \n",
    "        \n",
    "    return t_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "['group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week']\n"
     ]
    }
   ],
   "source": [
    "nut_tagged_sent = []\n",
    "\n",
    "for i, j in zip(dis_tagged_sent, ent_list_nut):\n",
    "    temp4 = tag_nut(i,j)\n",
    "    nut_tagged_sent.append(temp4)\n",
    "    \n",
    "print(len(nut_tagged_sent))\n",
    "print(nut_tagged_sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat two lists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for tag_sum_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25312\n",
      "['is', 'a', 'combination', 'of', 'controlled', 'research', 'observational', 'studies', 'and', 'field', 'experience', 'END', 'regardless', 'of', 'the', 'effect', 'on', 'rumen', 'epithelium', 'feedingdiets', 'containing', 'higher', 'proportions', 'of', 'B-NUT', 'should', 'promote', 'ruminal', 'microbial', 'adaptation', 'to', 'B-NUT', 'levels', 'typicalof', 'lactation', 'diets']\n"
     ]
    }
   ],
   "source": [
    "tag_sum = []\n",
    "\n",
    "sent_append = []\n",
    "\n",
    "tag_sum_mid = []\n",
    "\n",
    "for sent in nut_tagged_sent:\n",
    "    sent_temp = sent + ['END']\n",
    "    sent_append.append(sent_temp)\n",
    "\n",
    "for sent in sent_append:\n",
    "    tag_sum_mid += sent\n",
    "    \n",
    "print(len(tag_sum_mid))\n",
    "print(tag_sum_mid[35:71])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25312\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NUT', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# replace with 'O'\n",
    "\n",
    "final_tag = []\n",
    "for index, i in enumerate(tag_sum_mid):\n",
    "    if i != 'B-DIS' and i != 'I-DIS' and i != 'B-NUT' and i != 'I-NUT' and i != 'END':\n",
    "        i = 'O'\n",
    "    else:\n",
    "        i = tag_sum_mid[index]\n",
    "    \n",
    "    final_tag.append(i)\n",
    "\n",
    "print(len(final_tag))\n",
    "print(final_tag[35:71])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for origin sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n",
      "['regardless', 'of', 'the', 'effect', 'on', 'rumen', 'epithelium', 'feedingdiets', 'containing', 'higher', 'proportions', 'of', 'nfc', 'should', 'promote', 'ruminal', 'microbial', 'adaptation', 'to', 'nfc', 'levels', 'typicalof', 'lactation', 'diets']\n"
     ]
    }
   ],
   "source": [
    "### get the origin list \n",
    "\n",
    "def split_one(t_sent):\n",
    "    o_t_sent_split = t_sent.split(' ')\n",
    "    t_tag = [0] * len(o_t_sent_split)\n",
    "\n",
    "    # remove punct\n",
    "    t_sent_split = []\n",
    "    for i in o_t_sent_split:\n",
    "        i1 = i.replace(',','')\n",
    "        i2 = i1.replace('.','')\n",
    "        if i2 != '':\n",
    "            t_sent_split.append(i2)\n",
    "        \n",
    "    return t_sent_split\n",
    "\n",
    "t_sent_split = []\n",
    "for i in new_tokenize_text:\n",
    "    temp3 = split_one(i)\n",
    "    t_sent_split.append(temp3)\n",
    "    \n",
    "print(len(t_sent_split))\n",
    "print(t_sent_split[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25312\n",
      "['a', 'host', 'ofsignaling', 'molecules', 'are', 'released', 'by', 'activated', 'immune', 'cellsincluding', 'inflammatory', 'mediators', 'such', 'as', 'prostaglandinsand', 'cytokines', 'END', 'stocking', 'density', 'END', 'group', 'cow', 'relocations', 'to', 'only', 'take', 'place', 'once', 'a', 'week', 'END', 'thus', 'our', 'knowledge', 'base', 'is', 'a', 'combination', 'of', 'controlled', 'research', 'observational', 'studies', 'and', 'field', 'experience', 'END', 'regardless', 'of', 'the', 'effect', 'on', 'rumen', 'epithelium', 'feedingdiets', 'containing', 'higher', 'proportions', 'of', 'nfc', 'should', 'promote', 'ruminal', 'microbial', 'adaptation', 'to', 'nfc', 'levels', 'typicalof', 'lactation', 'diets', 'END', 'the', 'objectives', 'here', 'were', 'to', '1', 'identify', 'critical', 'thresholds', 'above', 'which', 'nefa', 'and', 'bhb', 'concentrations', 'increase', 'the', 'risk', 'of', 'disease', 'and', 'affect', 'production', 'and', 'reproductive', 'performance', 'at', 'the']\n"
     ]
    }
   ],
   "source": [
    "t_sent_split\n",
    "\n",
    "ori_append = []\n",
    "\n",
    "ori_sum_mid = []\n",
    "\n",
    "for k in t_sent_split:\n",
    "    ori_temp = k + ['END']\n",
    "    ori_append.append(ori_temp)\n",
    "\n",
    "for k in ori_append:\n",
    "    ori_sum_mid += k\n",
    "    \n",
    "print(len(ori_sum_mid))\n",
    "print(ori_sum_mid[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, final_tag\n",
    "# 2, ori_sum_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as words, labels and pos\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "df = pd.DataFrame([ori_sum_mid,final_tag])\n",
    "\n",
    "# transpose the columns and rows\n",
    "d_f = df.T\n",
    "\n",
    "# ========== Name of the output excel  ==========\n",
    "\n",
    "d_f.to_excel('training_set_excel_12_20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
